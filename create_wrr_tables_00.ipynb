{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smartsheet_dataframe import get_as_df\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Smartsheet information in variables\n",
    "shitz_token = \"wMvGniITjDFd7ClZPE44wtJGvZkM7Hg7mP7if\" #Bily's personal token to access G-A smartsheet data. Anyone can obtain their own token\n",
    "fhaid = 4523924143794052 # The ID number that references the FHA sheet\n",
    "mssid = 3005704744265604 # The ID number that references the MSS sheet\n",
    "\n",
    "# Creating the FHA and MSS dataframes\n",
    "fha_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=fhaid)\n",
    "\n",
    "mss_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=mssid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the WRR items\n",
    "wrr_df = mss_df.loc[mss_df['UniqueID'].str.contains(\"WRR\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make rough draft of the client table\n",
    "client_df = wrr_df[['UniqueID',\n",
    "                    'Tract',\n",
    "                    'CMP',\n",
    "                    'Stand',\n",
    "                    'Acres',\n",
    "                    'Age',\n",
    "                    'County',\n",
    "                    'State',\n",
    "                    'Stand Type',\n",
    "                    'Origin',\n",
    "                    'Scheduled',\n",
    "                    'PreBA',\n",
    "                    'PreTPA',\n",
    "                    'PreGTA',\n",
    "                    'PlnBA',\n",
    "                    'PlnTPA',\n",
    "                    'PlnGTA',\n",
    "                    'Client GT',\n",
    "                    'Future GT',\n",
    "                    'Reporting Period']].copy()\n",
    "\n",
    "# fix up Reporting Period column\n",
    "client_df['Reporting Period'] = client_df['Reporting Period'].fillna(0)\n",
    "client_df.loc[client_df['Reporting Period'] == \"\", 'Reporting Period'] = 0\n",
    "s = client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'].apply(lambda x: x.strip()[-1])\n",
    "client_df['trial'] = client_df.index.map(s)\n",
    "client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'] = client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'trial']\n",
    "client_df['Reporting Period'] = pd.to_numeric(client_df['Reporting Period'], downcast='integer')\n",
    "# Fix up age column\n",
    "client_df['Age'] = pd.to_numeric(client_df['Age'], errors='coerce', downcast='integer').fillna(0).astype(int)\n",
    "# fix up future gt column\n",
    "client_df.loc[pd.to_numeric(client_df['Future GT'], errors='coerce').isnull(), 'Future GT'] = 0\n",
    "# fix up Tract and Stand columns\n",
    "client_df['Tract'] = client_df['Tract'].astype(str)\n",
    "client_df['Stand'] = client_df['Stand'].astype(str)\n",
    "client_df['Tract'] = client_df['Tract'].apply(lambda x: x.split('.')[0])\n",
    "# fix up client gt column\n",
    "client_df['Client GT'] = pd.to_numeric(client_df['Client GT'], errors=\"coerce\")\n",
    "client_df['Client GT'] = client_df['Client GT'].fillna(0)\n",
    "\n",
    "# fix up harvest instake columns\n",
    "cols = ['PreBA', 'PreTPA', 'PreGTA', 'PlnBA', 'PlnTPA', 'PlnGTA']\n",
    "\n",
    "for x in cols:\n",
    "    client_df[x] = pd.to_numeric(client_df[x], errors='coerce', downcast='integer').fillna(-99).astype(int)\n",
    "\n",
    "client_df.drop(columns=['trial'],inplace=True)\n",
    "\n",
    "\n",
    "# Normalize species\n",
    "\n",
    "species = [\"Slash Pine\", \"Loblolly Pine\", 'Mixed Natural Pine', 'Hardwood Mix', 'Pine Straw', \"Other\"]\n",
    "species_dict = {'Species': species}\n",
    "species_df = pd.DataFrame(species_dict)\n",
    "species_df['SpeciesUID'] = species_df.index\n",
    "\n",
    "def species_map(x):\n",
    "    x = x.lower()\n",
    "    if \"slash\" in x:\n",
    "        return 0\n",
    "    elif \"loblol\" in x:\n",
    "        return 1\n",
    "    elif \"mixed natural\" in x:\n",
    "        return 2\n",
    "    elif \"h\" in x:\n",
    "        return 3\n",
    "    elif \"straw\" in x:\n",
    "        return 4\n",
    "    \n",
    "    elif \"pine\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "client_df['Stand Type'] = client_df['Stand Type'].apply(lambda x: species_map(x))\n",
    "\n",
    "# Normalize Origin\n",
    "\n",
    "origins = ['Planted', 'Natural', 'Natural Mix', 'Other']\n",
    "origin_dict = {'Origin': origins}\n",
    "origin_df = pd.DataFrame(origin_dict)\n",
    "origin_df['OriginUID'] = origin_df.index\n",
    "\n",
    "def origin_map(x):\n",
    "    x = x.lower()\n",
    "    if \"plant\" in x:\n",
    "        return 0\n",
    "    elif \"natural mix\" in x:\n",
    "        return 2\n",
    "    elif \"nat\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 3\n",
    "client_df['Origin'] = client_df['Origin'].apply(lambda x: origin_map(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "client_df.columns = [x.lower() for x in client_df.columns]\n",
    "client_df.columns = [x.replace(' ', '_') for x in client_df.columns]\n",
    "client_df['id'] = client_df['uniqueid']\n",
    "client_df.drop(columns=['uniqueid'],inplace=True)\n",
    "client_df = client_df[['id', 'tract', 'cmp', 'stand', 'acres', 'age', 'county', 'state', 'stand_type', 'origin', 'scheduled', 'preba', 'pretpa', 'pregta',\n",
    "                       'plnba', 'plntpa', 'plngta', 'client_gt', 'future_gt', 'reporting_period']]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_intake_df = wrr_df[['IntakeID',\n",
    "                       'FolderID',\n",
    "                       'UniqueID',\n",
    "                       'GA_ACRES',\n",
    "                       'GA GT', # this is going to be GA total isn't it? Or does it stay as GA estimate of current GT (total minus what client says is future)\n",
    "                       'Total GT', # Does this need included? At the moment we do funny stuff with total gt and subtracting future gt to get GA current rp (above)\n",
    "                       'Notes'\n",
    "                       ]].copy()\n",
    "\n",
    "# Do I want to include a Ticketid column here or use a crossreference table?\n",
    "# Should reporting period be in here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = wrr_df[['UniqueID',\n",
    "                    'Guidance',\n",
    "                    'Expiration',\n",
    "                    'GuidanceProvider',\n",
    "                    'Status',\n",
    "                    'Completed Date'\n",
    "                    ]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intake_breaker(lst):\n",
    "    intake_lst = []\n",
    "    activity_lst = []\n",
    "    for ele in lst:\n",
    "        intake_lst.append(ele[:9])\n",
    "        activity_lst.append(ele[9:11])\n",
    "        intake_st = set(intake_lst)\n",
    "        activity_st = set(activity_lst)\n",
    "        \n",
    "    return intake_lst, activity_lst, intake_st, activity_st\n",
    "\n",
    "\n",
    "def ticketid(lst):\n",
    "    \"\"\"This function creates a ticket id based off a list of unique ids\n",
    "\n",
    "    Args:\n",
    "        lst (list): a list of unique ids that have the same Tract_Cmp_Stand from MSS\n",
    "    \"\"\"\n",
    "    if len(lst) == 0:\n",
    "        return \"ERROR, Where is your uniqueid?\"\n",
    "    elif len(lst) < 2:\n",
    "        return lst[0]\n",
    "    elif len(lst) > 1:\n",
    "        # Check to see if there are multiple intakes\n",
    "        temp_lst, activity_lst, temp_st, activity_st = intake_breaker(lst)\n",
    "        temp_lst.sort()\n",
    "        activity_lst.sort()\n",
    "        if len(temp_st) > 1:\n",
    "            return f\"{temp_lst[0]}XXXX\"\n",
    "        # If there are not multiple intakes. Check to see if there are multiple activities in the lst (Intake portion should be same for all)\n",
    "        elif len(activity_st) > 1:\n",
    "           return f\"{temp_lst[0]}XXXX\"\n",
    "       # If there is only a single activity type\n",
    "        elif len(activity_st) == 1:\n",
    "            return f\"{temp_lst[0]}{activity_lst[0]}XX\"\n",
    "        else:\n",
    "            return \"Failed within len(lst) > 1\"\n",
    "        \n",
    "    else:\n",
    "        return \"Failed at length of lst\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_inprog = wrr_df[wrr_df['Status'].isin(['OPEN', 'IN PROCESS', 'POST', 'PENDING'])].copy()\n",
    "wrr_inprog['Tract'] = wrr_inprog['Tract'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "wrr_inprog['CMP'] = wrr_inprog['CMP'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "wrr_inprog['Stand'] = wrr_inprog['Stand'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "wrr_inprog['TCS'] = wrr_inprog['Tract'] + \"_\" + wrr_inprog['CMP'] + \"_\" + wrr_inprog['Stand']\n",
    "\n",
    "# After talking with Rob, we need to make sure that we separate out the GA and the ARB Guidance\n",
    "grouped_activities = wrr_inprog.groupby(['TCS', 'GuidanceProvider'])['UniqueID'].apply(list).reset_index()\n",
    "\n",
    "trial_df = grouped_activities.copy()\n",
    "trial_df['BBID'] = trial_df['UniqueID'].apply(lambda x: ticketid(x))\n",
    "\n",
    "prev_combined = wrr_inprog.groupby('TicketID')['TCS'].apply(set).reset_index()\n",
    "\n",
    "tcs = []\n",
    "for val in prev_combined['TCS'].loc[prev_combined['TCS'].str.len()> 1].values:\n",
    "    for item in val:\n",
    "        tcs.append(item)\n",
    "\n",
    "# looking for TCS combinations in previous ticketids        \n",
    "ticket_id_dict = {}\n",
    "for i, row in prev_combined.loc[prev_combined['TCS'].str.len()> 1].iterrows():\n",
    "    # print(row['TicketID']) # ticket id\n",
    "    value_list = []\n",
    "    for value in row['TCS']:\n",
    "        value_list.append(value)\n",
    "    ticket_id_dict[row['TicketID']] = value_list\n",
    "if len(ticket_id_dict) > 0:\n",
    "    for k, v in ticket_id_dict.items():\n",
    "        trial_df.loc[trial_df['TCS'].isin(v), 'otherid'] = k\n",
    "else:\n",
    "    trial_df['otherid'] = None\n",
    "    \n",
    "\n",
    "def final_id(row):\n",
    "    if row['otherid'] != None:\n",
    "        return row['otherid']\n",
    "    else:\n",
    "        return row['BBID']\n",
    "    \n",
    "trial_df['finalid'] = trial_df[['BBID', 'otherid']].apply(final_id, axis=1)\n",
    "\n",
    "def break_list(x):\n",
    "    trial_list = []\n",
    "    for item in x:\n",
    "        for iter in item:\n",
    "            trial_list.append(iter)\n",
    "            \n",
    "    return trial_list\n",
    "\n",
    "new_ticket_df = trial_df.groupby('finalid').agg({'TCS': lambda x: list(x),\n",
    "                                 'UniqueID': lambda x: break_list(x)}).reset_index()\n",
    "\n",
    "client_for_ticket = client_df.copy()\n",
    "for id in new_ticket_df['finalid'].unique().tolist():\n",
    "    client_for_ticket.loc[client_for_ticket['UniqueID'].isin(new_ticket_df.loc[new_ticket_df['finalid'] == id, 'UniqueID'].values[0]), 'TicketID'] = id\n",
    "\n",
    "new_tcs_intake = client_for_ticket.groupby(['TicketID',\n",
    "                                            'UniqueID'])['Client GT'].sum().reset_index()\n",
    "\n",
    "old_ticket_ids = dict(zip(wrr_df['UniqueID'], wrr_df['TicketID']))\n",
    "new_tcs_intake['old_ticketids'] = new_tcs_intake['UniqueID'].map(old_ticket_ids)\n",
    "new_tcs_intake.loc[~new_tcs_intake['TicketID'].str.contains(\"WRR\"), 'TicketID'] = new_tcs_intake['old_ticketids']\n",
    "\n",
    "\n",
    "ticketid_table = new_tcs_intake[['TicketID', 'UniqueID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the mill tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_loc = r\"P:\\Mill Tickets\"\n",
    "file_pattern = \"WRR_RP*_Compiled.xlsm\"\n",
    "big_df = pd.DataFrame()\n",
    "\n",
    "mill_name = []\n",
    "for root, dirs, files in os.walk(mill_loc):\n",
    "    for file in files:\n",
    "        if \"WRR_RP\" in file and file.endswith(\"Compiled.xlsm\"):\n",
    "            if \"$\" in file:\n",
    "                pass\n",
    "            else:\n",
    "                print(root)\n",
    "                print(file)\n",
    "                df = pd.read_excel(os.path.join(root, file), sheet_name='RP_Project')\n",
    "                big_df = pd.concat([big_df, df])\n",
    "                \n",
    "\n",
    "# big_df.loc[big_df['Tons (U.S)'].isnull(), 'Tons (U.S)'] = big_df.loc[big_df['Tons (U.S)'].isnull(), 'TonsUS']\n",
    "# big_df['Cull Tons'].fillna(big_df['CullTons'], inplace=True)\n",
    "# big_df['Accepted Tons'].fillna(big_df['AcceptedTons'], inplace=True)\n",
    "# big_df['Reporting Period'].fillna(big_df['ReportingPeriod'],inplace=True)\n",
    "\n",
    "def rep_fix(x):\n",
    "    if \"rp\" in str(x).lower():\n",
    "        return x[-1]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "big_df['Reporting Period'] = big_df['Reporting Period'].apply(lambda x: rep_fix(x))\n",
    "big_df['Reporting Period'] = pd.to_numeric(big_df['Reporting Period'], errors='coerce')\n",
    "\n",
    "big_df['Date'] = pd.to_datetime(big_df['Date'])\n",
    "\n",
    "mill_df = big_df[['Date',\n",
    "                  'Ticket',\n",
    "                  'Mill',\n",
    "                  'Product',\n",
    "                  'Tons (U.S)',\n",
    "                  'Cull Tons',\n",
    "                  'Accepted Tons',\n",
    "                  'Source',\n",
    "                  'Notes',\n",
    "                  'Reporting Period',\n",
    "                  'TicketID',\n",
    "                  'Week']].copy()\n",
    "\n",
    "mill_df = mill_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the old Harvest TicketIDs\n",
    "harvested_tons_oldTID = mill_df[mill_df['Reporting Period'] == 7].groupby(\"TicketID\")['Tons (U.S)'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvested_tons_oldTID['Tons (U.S)'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get new and old TIDs that are currently in process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting harvest ids that were in RP 7 and are still \"Active\" (Potentially need updating due to harvest)\n",
    "harvest_activities = wrr_df[(wrr_df['Reporting Period'] == 7) & (wrr_df['Status'].isin(['IN PROCESS', \"POST\"]))]['UniqueID'].unique().tolist()\n",
    "harvest_old_ticketid = wrr_df[(wrr_df['Reporting Period'] == 7) & (wrr_df['Status'].isin(['IN PROCESS', \"POST\"]))]['TicketID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many tons are under old TIDs of activities in question\n",
    "harvested_tons_oldTID[harvested_tons_oldTID['TicketID'].isin(harvest_old_ticketid)]['Tons (U.S)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the new TIDs based on these harvest activities\n",
    "new_ticketIDs = ticketid_table[ticketid_table['UniqueID'].isin(harvest_activities)]['TicketID'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what is in old_ticketid that is not in new_ticketIDs\n",
    "[item for item in harvest_old_ticketid if item not in new_ticketIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in harvest_old_ticketid:\n",
    "    old_unique = wrr_df[wrr_df['TicketID'] == id]['UniqueID'].unique().tolist()\n",
    "    new_unique = ticketid_table[ticketid_table['TicketID'] == id]['UniqueID'].unique().tolist()\n",
    "    inold_notinew = [item for item in old_unique if item not in new_unique]\n",
    "    if len(inold_notinew) > 0:\n",
    "        print(id, old_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so some old ticketids don't line up with the new ticket ids\n",
    "\n",
    "Let's update the mill compiled file and replace the ticket ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df[mill_df['TicketID'].isin(['WRR231107CC02', 'WRR231107TT01'])]['TicketID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df[mill_df['TicketID'].str.contains(\"WRR231107CC02\")]['TicketID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game plan is to get the old uniqueid -> ticketid connection and then find the new ticketid.\n",
    "# Then replace all ticketids in the mill df to the new using a map\n",
    "\n",
    "wrr_df_uid_tid = wrr_df[['UniqueID', 'TicketID']].copy()\n",
    "wrr_df_uid_tid = pd.merge(wrr_df_uid_tid, ticketid_table, on='UniqueID', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['UniqueID'].isin(['WRR231107CC02', 'WRR231107TT01'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_df_uid_tid[wrr_df_uid_tid['TicketID_y'].str.contains('X')]\n",
    "wrr_df_tids = wrr_df_uid_tid[['TicketID_x', 'TicketID_y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_df_tids.drop_duplicates(inplace=True)\n",
    "wrr_tid_mapper = dict(zip(wrr_df_tids['TicketID_x'], wrr_df_tids['TicketID_y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df['new_ticketid'] = mill_df['TicketID'].map(wrr_tid_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ticket_harvested = mill_df.groupby('new_ticketid')['Tons (U.S)'].sum().reset_index()\n",
    "new_ticket_harvested['Tons (U.S)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df['Tons (U.S)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df[mill_df['new_ticketid'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There was an issue where WRR230220HD01 was mislabeled WRR230220CC01 in the RP7 update compiled. I fixed that at the source on 2024.11.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harvested_tons_oldTID.to_excel(\"harvested_tons_compare_old.xlsx\", sheet_name='oldTID')\n",
    "# new_ticket_harvested.to_excel(\"harvested_tons_compare_new.xlsx\", sheet_name='newTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_expected = pd.merge(ticketid_table, wrr_df[['UniqueID', 'Current RP Expected GT', 'Status', 'Future GT']], on='UniqueID', how='left')\n",
    "trial_expected = pd.merge(trial_expected, client_df[['UniqueID', 'Reporting Period']], on='UniqueID', how='left')\n",
    "wrr_rp7_expected = trial_expected[(trial_expected['Reporting Period'] == 7) & trial_expected['Status'].isin(['IN PROCESS', 'OPEN', 'POST'])].groupby('TicketID')['Current RP Expected GT'].sum().reset_index()\n",
    "wrr_rp7_expected = pd.merge(wrr_rp7_expected, harvested_tons_oldTID, on='TicketID', how='left')\n",
    "wrr_rp7_expected['Tons (U.S)'] = pd.to_numeric(wrr_rp7_expected['Tons (U.S)'], errors='coerce', downcast='float')\n",
    "wrr_rp7_expected['Current RP Expected GT'] = pd.to_numeric(wrr_rp7_expected['Current RP Expected GT'], errors='coerce', downcast='float')\n",
    "wrr_rp7_expected['Tons (U.S)'].fillna(0, inplace=True)\n",
    "wrr_rp7_expected['Current RP Expected GT'].fillna(0, inplace=True)\n",
    "wrr_rp7_expected['Remaining'] = wrr_rp7_expected['Current RP Expected GT'] - wrr_rp7_expected['Tons (U.S)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp7_expected[wrr_rp7_expected['Remaining'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrr_rp7_expected.to_excel(\"wrr_rp7_accouting.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_expected = trial_expected[(trial_expected['Reporting Period'] == 8) & (trial_expected['Status'].isin(['OPEN', 'IN PROCESS', 'PENDING', 'POST']))].groupby('TicketID')['Current RP Expected GT'].sum().reset_index()\n",
    "wrr_rp8_expected_1 = trial_expected[(trial_expected['Reporting Period'] == 7) & (trial_expected['Status'].isin(['OPEN', 'IN PROCESS', 'PENDING', 'POST']))].groupby('TicketID')['Future GT'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_expected = pd.merge(wrr_rp8_expected, wrr_rp8_expected_1, on='TicketID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_expected['Current RP Expected GT'] = pd.to_numeric(wrr_rp8_expected['Current RP Expected GT'], errors='coerce', downcast='float')\n",
    "wrr_rp8_expected['Future GT'] = pd.to_numeric(wrr_rp8_expected['Future GT'], errors='coerce', downcast='float')\n",
    "wrr_rp8_expected.fillna(0, inplace=True)\n",
    "wrr_rp8_expected['RP8_Expected'] = wrr_rp8_expected['Current RP Expected GT'] + wrr_rp8_expected['Future GT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_trial = pd.merge(wrr_rp8_expected, wrr_rp7_expected, on='TicketID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_trial\n",
    "wrr_rp8_trial.fillna(0, inplace=True)\n",
    "wrr_rp8_trial['final_expected'] = wrr_rp8_trial['RP8_Expected'] + wrr_rp8_trial['Remaining']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, this adds up with Rob's file. Lets get the info needed to fill out the Shared Space table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_ticketid_volume = wrr_rp8_trial[['TicketID', 'final_expected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_mapper = dict(zip(ticketid_table['UniqueID'], ticketid_table['TicketID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_df['new_ticket_id'] = wrr_df['UniqueID'].map(ticket_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrr_rp8_trial.to_excel('wrr_rp8_investigate_3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table.loc[ticketid_table['TicketID'] == 'WRR231107XXXX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok. Let's create the client facing table and get it up on the drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_intake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df.head() # make sure to use the correct ticketid long term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tcs_intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_inprog[['UniqueID', 'TCS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog = status_df[status_df['Status'].isin(['OPEN', 'IN PROCESS', 'POST'])].copy()\n",
    "client_table_inprog = pd.merge(client_table_inprog, wrr_inprog, on='UniqueID', how='left')\n",
    "client_table_inprog = pd.merge(client_table_inprog, ticketid_table, on='UniqueID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog = client_table_inprog[['UniqueID', 'TCS', 'Status_x', 'Guidance_x', 'TicketID_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_aligned_with_rob = pd.read_excel(\"./WRR_RP8_InProcess/wrr_rp8_investigate_3.xlsx\")\n",
    "expected_aligned_with_rob = expected_aligned_with_rob[['TicketID', 'Final Values']]\n",
    "expected_aligned_with_rob['RP8_Expected'] = pd.to_numeric(expected_aligned_with_rob['Final Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog = pd.merge(client_table_inprog, expected_aligned_with_rob, left_on='TicketID_y', right_on='TicketID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_final_df = client_table_inprog.groupby('TicketID').agg({'TCS': \", \".join,\n",
    "                                               'Status_x': \", \".join,\n",
    "                                               'Guidance_x': \", \".join,\n",
    "                                               'RP8_Expected': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_final_df['RP8_Expected'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I totally did not make the expected by RP table\n",
    "\n",
    "I need to get total expected by activity\n",
    "\n",
    "and\n",
    "\n",
    "I need to get the expected by RP\n",
    "\n",
    "Fortunately WRR doesn't have any \"Future\" rp volumes to worry about - just carry over, and we are only worrying about RP7 -> RP8 for that\n",
    "\n",
    "and RP8 is figured out already from above.\n",
    "\n",
    "So, we just need to grab all the previous volumes to build out the individual RP expected volumes.\n",
    "\n",
    "The trick will be finding the total? Or should we just base it off the total in mss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_df[['UniqueID', 'Reporting Period', 'Current RP Expected GT', 'Total GT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog = wrr_df[wrr_df['Reporting Period'].notna()][['UniqueID', 'Reporting Period', 'Current RP Expected GT', 'Total GT']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog['Reporting Period'].fillna(0, inplace=True)\n",
    "wrr_expected_inprog.loc[client_df['Reporting Period'] == \"\", 'Reporting Period'] = 0\n",
    "s = wrr_expected_inprog.loc[pd.to_numeric(wrr_expected_inprog['Reporting Period'], errors='coerce').isnull() & wrr_expected_inprog['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'].apply(lambda x: x.strip()[-1])\n",
    "wrr_expected_inprog['trial'] = wrr_expected_inprog.index.map(s)\n",
    "wrr_expected_inprog.loc[pd.to_numeric(wrr_expected_inprog['Reporting Period'], errors='coerce').isnull() & wrr_expected_inprog['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'] = wrr_expected_inprog.loc[pd.to_numeric(wrr_expected_inprog['Reporting Period'], errors='coerce').isnull() & wrr_expected_inprog['Reporting Period'].str.contains(\"RP\"), 'trial']\n",
    "wrr_expected_inprog['Reporting Period'] = pd.to_numeric(wrr_expected_inprog['Reporting Period'], downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog['Current RP Expected GT'] = pd.to_numeric(wrr_expected_inprog['Current RP Expected GT'], errors='coerce')\n",
    "wrr_expected_inprog['Total GT'] = pd.to_numeric(wrr_expected_inprog['Total GT'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog = wrr_expected_inprog[((wrr_expected_inprog['Current RP Expected GT'].notna()) | (wrr_expected_inprog['Total GT'].notna())) & (wrr_expected_inprog['Reporting Period'] < 8)]\n",
    "wrr_expected_inprog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, well.. Looks like it is all RP7 - but that is the data we have.\n",
    "\n",
    "Time to tie in the TicketID and then groupby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog = pd.merge(wrr_expected_inprog, ticketid_table, on='UniqueID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected = wrr_expected_inprog.groupby(['TicketID', 'Reporting Period'])['Current RP Expected GT'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp8_expect = client_final_df[['TicketID', 'RP8_Expected']].copy()\n",
    "rp8_expect['Reporting Period'] = 8\n",
    "rp8_expect['Current RP Expected GT'] = rp8_expect['RP8_Expected']\n",
    "rp8_expect = rp8_expect[['TicketID', 'Current RP Expected GT', 'Reporting Period']].copy()\n",
    "wrr_expected =pd.concat([wrr_expected, rp8_expect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the aggregation of \"total gt\" for each new ticketid? \n",
    "\n",
    "After some looking - it looks like we don't need to worry about this for the current rp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_expect_inprog = wrr_df[['UniqueID', 'Total GT']].copy()\n",
    "total_expect_inprog = pd.merge(total_expect_inprog, ticketid_table, on='UniqueID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_expect_inprog['Total GT'] = pd.to_numeric(total_expect_inprog['Total GT'], errors='coerce')\n",
    "total_expect_interest = total_expect_inprog[total_expect_inprog['Total GT'] > 0].copy()\n",
    "ticket_total = total_expect_interest.groupby('TicketID')['Total GT'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_total.to_excel('first_ticket_total.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the harvested volumes for RP8 and total then for these magical beasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mill_rp = mill_df.groupby(['new_ticketid', 'Reporting Period'])['Tons (U.S)'].sum().reset_index()\n",
    "grouped_mill_total = mill_df.groupby('new_ticketid')['Tons (U.S)'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# able to get either at rp level or total for ticket id\n",
    "# new_ticket_total = pd.merge(ticket_total, grouped_mill_rp[grouped_mill_rp['Reporting Period'] > 7], left_on='TicketID', right_on='new_ticketid', how='inner')\n",
    "new_ticket_total = pd.merge(ticket_total, grouped_mill_total, left_on='TicketID', right_on='new_ticketid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_ticket_total.to_excel('total_volume.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the last ticket date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_recentdate = mill_df.groupby('new_ticketid')['Date'].last().reset_index()\n",
    "# ticket_recentdate.to_excel('ticketdate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['UniqueID'] == 'WRR240101CC01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['TicketID'].str.contains('X')]['TicketID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['TicketID'] == 'WRR231107XXXX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df[mill_df['TicketID'] == 'WRR231107CC02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lst = ticketid_table['UniqueID'].unique().tolist()\n",
    "len(wrr_df[(wrr_df['UniqueID'].isin(unique_lst)) & (wrr_df['Status'].isin(['IN PROCESS', 'OPEN', 'POST']))]['TicketID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inprog_wrr_df = wrr_df[wrr_df['Status'].isin(['IN PROCESS', 'POST'])].copy()\n",
    "inprog_wrr_oldTID = inprog_wrr_df.groupby(['TicketID'])['UniqueID'].apply(list).reset_index()\n",
    "# inprog_wrr_oldTID.to_excel(\"old_tid_unique.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['TicketID'].isin(['WRR230829CCXX',\n",
    "                                                'WRR230829TH01',\n",
    "                                                'WRR230829TH02',\n",
    "                                                'WRR231107CC01',\n",
    "                                                'WRR231107CC03',\n",
    "                                                'WRR231107TH01',\n",
    "                                                'WRR231107XXXX',\n",
    "                                                'WRR240101TH01',\n",
    "                                                'WRR240101TT01',\n",
    "                                                'WRR240101XXXX',\n",
    "                                                'WRR240404TH01',\n",
    "                                                'WRR240404TH02',\n",
    "                                                'WRR240709CCXX',\n",
    "                                                'WRR240802TH01',\n",
    "                                                'WRR240802TH03'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgres_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
