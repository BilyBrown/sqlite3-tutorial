{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smartsheet_dataframe import get_as_df\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Smartsheet information in variables\n",
    "shitz_token = \"wMvGniITjDFd7ClZPE44wtJGvZkM7Hg7mP7if\" #Bily's personal token to access G-A smartsheet data. Anyone can obtain their own token\n",
    "fhaid = 4523924143794052 # The ID number that references the FHA sheet\n",
    "mssid = 3005704744265604 # The ID number that references the MSS sheet\n",
    "\n",
    "# Creating the FHA and MSS dataframes\n",
    "fha_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=fhaid)\n",
    "\n",
    "mss_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=mssid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the WRR items\n",
    "wrr_df = mss_df.loc[mss_df['UniqueID'].str.contains(\"WRR\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make rough draft of the client table\n",
    "client_df = wrr_df[['UniqueID',\n",
    "                    'Tract',\n",
    "                    'CMP',\n",
    "                    'Stand',\n",
    "                    'Acres',\n",
    "                    'Age',\n",
    "                    'County',\n",
    "                    'State',\n",
    "                    'Stand Type',\n",
    "                    'Origin',\n",
    "                    'Scheduled',\n",
    "                    'PreBA',\n",
    "                    'PreTPA',\n",
    "                    'PreGTA',\n",
    "                    'PlnBA',\n",
    "                    'PlnTPA',\n",
    "                    'PlnGTA',\n",
    "                    'Client GT',\n",
    "                    'Future GT',\n",
    "                    'Reporting Period']].copy()\n",
    "\n",
    "# fix up Reporting Period column\n",
    "client_df['Reporting Period'] = client_df['Reporting Period'].fillna(0)\n",
    "client_df.loc[client_df['Reporting Period'] == \"\", 'Reporting Period'] = 0\n",
    "s = client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'].apply(lambda x: x.strip()[-1])\n",
    "client_df['trial'] = client_df.index.map(s)\n",
    "client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'] = client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'trial']\n",
    "client_df['Reporting Period'] = pd.to_numeric(client_df['Reporting Period'], downcast='integer')\n",
    "# Fix up age column\n",
    "client_df['Age'] = pd.to_numeric(client_df['Age'], errors='coerce', downcast='integer').fillna(0).astype(int)\n",
    "# fix up future gt column\n",
    "client_df.loc[pd.to_numeric(client_df['Future GT'], errors='coerce').isnull(), 'Future GT'] = 0\n",
    "# fix up Tract and Stand columns\n",
    "client_df['Tract'] = client_df['Tract'].astype(str)\n",
    "client_df['Stand'] = client_df['Stand'].astype(str)\n",
    "client_df['Tract'] = client_df['Tract'].apply(lambda x: x.split('.')[0])\n",
    "# fix up client gt column\n",
    "client_df['Client GT'] = pd.to_numeric(client_df['Client GT'], errors=\"coerce\")\n",
    "client_df['Client GT'] = client_df['Client GT'].fillna(0)\n",
    "\n",
    "# fix up harvest instake columns\n",
    "cols = ['PreBA', 'PreTPA', 'PreGTA', 'PlnBA', 'PlnTPA', 'PlnGTA']\n",
    "\n",
    "for x in cols:\n",
    "    client_df[x] = pd.to_numeric(client_df[x], errors='coerce', downcast='integer').fillna(-99).astype(int)\n",
    "\n",
    "client_df.drop(columns=['trial'],inplace=True)\n",
    "\n",
    "\n",
    "# Normalize species\n",
    "\n",
    "species = [\"Slash Pine\", \"Loblolly Pine\", 'Mixed Natural Pine', 'Hardwood Mix', 'Pine Straw', \"Other\"]\n",
    "species_dict = {'Species': species}\n",
    "species_df = pd.DataFrame(species_dict)\n",
    "species_df['SpeciesUID'] = species_df.index\n",
    "\n",
    "def species_map(x):\n",
    "    x = x.lower()\n",
    "    if \"slash\" in x:\n",
    "        return 0\n",
    "    elif \"loblol\" in x:\n",
    "        return 1\n",
    "    elif \"mixed natural\" in x:\n",
    "        return 2\n",
    "    elif \"h\" in x:\n",
    "        return 3\n",
    "    elif \"straw\" in x:\n",
    "        return 4\n",
    "    \n",
    "    elif \"pine\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "client_df['Stand Type'] = client_df['Stand Type'].apply(lambda x: species_map(x))\n",
    "\n",
    "# Normalize Origin\n",
    "\n",
    "origins = ['Planted', 'Natural', 'Natural Mix', 'Other']\n",
    "origin_dict = {'Origin': origins}\n",
    "origin_df = pd.DataFrame(origin_dict)\n",
    "origin_df['OriginUID'] = origin_df.index\n",
    "\n",
    "def origin_map(x):\n",
    "    x = x.lower()\n",
    "    if \"plant\" in x:\n",
    "        return 0\n",
    "    elif \"natural mix\" in x:\n",
    "        return 2\n",
    "    elif \"nat\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 3\n",
    "client_df['Origin'] = client_df['Origin'].apply(lambda x: origin_map(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclient_df.columns = [x.lower() for x in client_df.columns]\\nclient_df.columns = [x.replace(' ', '_') for x in client_df.columns]\\nclient_df['id'] = client_df['uniqueid']\\nclient_df.drop(columns=['uniqueid'],inplace=True)\\nclient_df = client_df[['id', 'tract', 'cmp', 'stand', 'acres', 'age', 'county', 'state', 'stand_type', 'origin', 'scheduled', 'preba', 'pretpa', 'pregta',\\n                       'plnba', 'plntpa', 'plngta', 'client_gt', 'future_gt', 'reporting_period']]\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "client_df.columns = [x.lower() for x in client_df.columns]\n",
    "client_df.columns = [x.replace(' ', '_') for x in client_df.columns]\n",
    "client_df['id'] = client_df['uniqueid']\n",
    "client_df.drop(columns=['uniqueid'],inplace=True)\n",
    "client_df = client_df[['id', 'tract', 'cmp', 'stand', 'acres', 'age', 'county', 'state', 'stand_type', 'origin', 'scheduled', 'preba', 'pretpa', 'pregta',\n",
    "                       'plnba', 'plntpa', 'plngta', 'client_gt', 'future_gt', 'reporting_period']]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_intake_df = wrr_df[['IntakeID',\n",
    "                       'FolderID',\n",
    "                       'UniqueID',\n",
    "                       'GA_ACRES',\n",
    "                       'GA GT', # this is going to be GA total isn't it? Or does it stay as GA estimate of current GT (total minus what client says is future)\n",
    "                       'Total GT', # Does this need included? At the moment we do funny stuff with total gt and subtracting future gt to get GA current rp (above)\n",
    "                       'Notes'\n",
    "                       ]].copy()\n",
    "\n",
    "# Do I want to include a Ticketid column here or use a crossreference table?\n",
    "# Should reporting period be in here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = wrr_df[['UniqueID',\n",
    "                    'Guidance',\n",
    "                    'Expiration',\n",
    "                    'GuidanceProvider',\n",
    "                    'Status',\n",
    "                    'Completed Date'\n",
    "                    ]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intake_breaker(lst):\n",
    "    intake_lst = []\n",
    "    activity_lst = []\n",
    "    for ele in lst:\n",
    "        intake_lst.append(ele[:9])\n",
    "        activity_lst.append(ele[9:11])\n",
    "        intake_st = set(intake_lst)\n",
    "        activity_st = set(activity_lst)\n",
    "        \n",
    "    return intake_lst, activity_lst, intake_st, activity_st\n",
    "\n",
    "\n",
    "def ticketid(lst):\n",
    "    \"\"\"This function creates a ticket id based off a list of unique ids\n",
    "\n",
    "    Args:\n",
    "        lst (list): a list of unique ids that have the same Tract_Cmp_Stand from MSS\n",
    "    \"\"\"\n",
    "    if len(lst) == 0:\n",
    "        return \"ERROR, Where is your uniqueid?\"\n",
    "    elif len(lst) < 2:\n",
    "        return lst[0]\n",
    "    elif len(lst) > 1:\n",
    "        # Check to see if there are multiple intakes\n",
    "        temp_lst, activity_lst, temp_st, activity_st = intake_breaker(lst)\n",
    "        temp_lst.sort()\n",
    "        activity_lst.sort()\n",
    "        if len(temp_st) > 1:\n",
    "            return f\"{temp_lst[0]}XXXX\"\n",
    "        # If there are not multiple intakes. Check to see if there are multiple activities in the lst (Intake portion should be same for all)\n",
    "        elif len(activity_st) > 1:\n",
    "           return f\"{temp_lst[0]}XXXX\"\n",
    "       # If there is only a single activity type\n",
    "        elif len(activity_st) == 1:\n",
    "            return f\"{temp_lst[0]}{activity_lst[0]}XX\"\n",
    "        else:\n",
    "            return \"Failed within len(lst) > 1\"\n",
    "        \n",
    "    else:\n",
    "        return \"Failed at length of lst\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_inprog = wrr_df[wrr_df['Status'].isin(['OPEN', 'IN PROCESS', 'POST', 'PENDING'])].copy()\n",
    "wrr_inprog['Tract'] = wrr_inprog['Tract'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "wrr_inprog['CMP'] = wrr_inprog['CMP'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "wrr_inprog['Stand'] = wrr_inprog['Stand'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "wrr_inprog['TCS'] = wrr_inprog['Tract'] + \"_\" + wrr_inprog['CMP'] + \"_\" + wrr_inprog['Stand']\n",
    "\n",
    "# After talking with Rob, we need to make sure that we separate out the GA and the ARB Guidance\n",
    "grouped_activities = wrr_inprog.groupby(['TCS', 'GuidanceProvider'])['UniqueID'].apply(list).reset_index()\n",
    "\n",
    "trial_df = grouped_activities.copy()\n",
    "trial_df['BBID'] = trial_df['UniqueID'].apply(lambda x: ticketid(x))\n",
    "\n",
    "prev_combined = wrr_inprog.groupby('TicketID')['TCS'].apply(set).reset_index()\n",
    "\n",
    "tcs = []\n",
    "for val in prev_combined['TCS'].loc[prev_combined['TCS'].str.len()> 1].values:\n",
    "    for item in val:\n",
    "        tcs.append(item)\n",
    "\n",
    "# looking for TCS combinations in previous ticketids        \n",
    "ticket_id_dict = {}\n",
    "for i, row in prev_combined.loc[prev_combined['TCS'].str.len()> 1].iterrows():\n",
    "    # print(row['TicketID']) # ticket id\n",
    "    value_list = []\n",
    "    for value in row['TCS']:\n",
    "        value_list.append(value)\n",
    "    ticket_id_dict[row['TicketID']] = value_list\n",
    "if len(ticket_id_dict) > 0:\n",
    "    for k, v in ticket_id_dict.items():\n",
    "        trial_df.loc[trial_df['TCS'].isin(v), 'otherid'] = k\n",
    "else:\n",
    "    trial_df['otherid'] = None\n",
    "    \n",
    "\n",
    "def final_id(row):\n",
    "    if row['otherid'] != None:\n",
    "        return row['otherid']\n",
    "    else:\n",
    "        return row['BBID']\n",
    "    \n",
    "trial_df['finalid'] = trial_df[['BBID', 'otherid']].apply(final_id, axis=1)\n",
    "\n",
    "def break_list(x):\n",
    "    trial_list = []\n",
    "    for item in x:\n",
    "        for iter in item:\n",
    "            trial_list.append(iter)\n",
    "            \n",
    "    return trial_list\n",
    "\n",
    "new_ticket_df = trial_df.groupby('finalid').agg({'TCS': lambda x: list(x),\n",
    "                                 'UniqueID': lambda x: break_list(x)}).reset_index()\n",
    "\n",
    "client_for_ticket = client_df.copy()\n",
    "for id in new_ticket_df['finalid'].unique().tolist():\n",
    "    client_for_ticket.loc[client_for_ticket['UniqueID'].isin(new_ticket_df.loc[new_ticket_df['finalid'] == id, 'UniqueID'].values[0]), 'TicketID'] = id\n",
    "\n",
    "new_tcs_intake = client_for_ticket.groupby(['TicketID',\n",
    "                                            'UniqueID'])['Client GT'].sum().reset_index()\n",
    "\n",
    "old_ticket_ids = dict(zip(wrr_df['UniqueID'], wrr_df['TicketID']))\n",
    "new_tcs_intake['old_ticketids'] = new_tcs_intake['UniqueID'].map(old_ticket_ids)\n",
    "new_tcs_intake.loc[~new_tcs_intake['TicketID'].str.contains(\"WRR\"), 'TicketID'] = new_tcs_intake['old_ticketids']\n",
    "\n",
    "\n",
    "ticketid_table = new_tcs_intake[['TicketID', 'UniqueID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the mill tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\ARCHIVE\\WRR_RP5\n",
      "WRR_RP5_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WRR_RP6\n",
      "WRR_RP6_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WRR_RP7\n",
      "WRR_RP7_Compiled.xlsm\n",
      "P:\\Mill Tickets\\WRR_RP8\n",
      "WRR_RP8_Compiled.xlsm\n"
     ]
    }
   ],
   "source": [
    "mill_loc = r\"P:\\Mill Tickets\"\n",
    "file_pattern = \"WRR_RP*_Compiled.xlsm\"\n",
    "big_df = pd.DataFrame()\n",
    "\n",
    "mill_name = []\n",
    "for root, dirs, files in os.walk(mill_loc):\n",
    "    for file in files:\n",
    "        if \"WRR_RP\" in file and file.endswith(\"Compiled.xlsm\"):\n",
    "            if \"$\" in file:\n",
    "                pass\n",
    "            else:\n",
    "                print(root)\n",
    "                print(file)\n",
    "                df = pd.read_excel(os.path.join(root, file), sheet_name='RP_Project')\n",
    "                big_df = pd.concat([big_df, df])\n",
    "                \n",
    "\n",
    "# big_df.loc[big_df['Tons (U.S)'].isnull(), 'Tons (U.S)'] = big_df.loc[big_df['Tons (U.S)'].isnull(), 'TonsUS']\n",
    "# big_df['Cull Tons'].fillna(big_df['CullTons'], inplace=True)\n",
    "# big_df['Accepted Tons'].fillna(big_df['AcceptedTons'], inplace=True)\n",
    "# big_df['Reporting Period'].fillna(big_df['ReportingPeriod'],inplace=True)\n",
    "\n",
    "def rep_fix(x):\n",
    "    if \"rp\" in str(x).lower():\n",
    "        return x[-1]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "big_df['Reporting Period'] = big_df['Reporting Period'].apply(lambda x: rep_fix(x))\n",
    "big_df['Reporting Period'] = pd.to_numeric(big_df['Reporting Period'], errors='coerce')\n",
    "\n",
    "big_df['Date'] = pd.to_datetime(big_df['Date'])\n",
    "\n",
    "mill_df = big_df[['Date',\n",
    "                  'Ticket',\n",
    "                  'Mill',\n",
    "                  'Product',\n",
    "                  'Tons (U.S)',\n",
    "                  'Cull Tons',\n",
    "                  'Accepted Tons',\n",
    "                  'Source',\n",
    "                  'Notes',\n",
    "                  'Reporting Period',\n",
    "                  'TicketID',\n",
    "                  'Week']].copy()\n",
    "\n",
    "mill_df = mill_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Mill</th>\n",
       "      <th>Product</th>\n",
       "      <th>Tons (U.S)</th>\n",
       "      <th>Cull Tons</th>\n",
       "      <th>Accepted Tons</th>\n",
       "      <th>Source</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Reporting Period</th>\n",
       "      <th>TicketID</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>875937</td>\n",
       "      <td>Georgia-Pacific Corporation - Cedar Springs Pa...</td>\n",
       "      <td>PPW</td>\n",
       "      <td>23.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.27</td>\n",
       "      <td>Mayhaw_Week 10.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>WRR210317XXXX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>875841</td>\n",
       "      <td>Georgia-Pacific Corporation - Cedar Springs Pa...</td>\n",
       "      <td>PPW</td>\n",
       "      <td>23.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.86</td>\n",
       "      <td>Mayhaw_Week 10.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>WRR210317XXXX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>875787</td>\n",
       "      <td>Georgia-Pacific Corporation - Cedar Springs Pa...</td>\n",
       "      <td>PPW</td>\n",
       "      <td>23.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.99</td>\n",
       "      <td>Mayhaw_Week 10.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>WRR210317XXXX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>876023</td>\n",
       "      <td>Georgia-Pacific Corporation - Cedar Springs Pa...</td>\n",
       "      <td>PPW</td>\n",
       "      <td>28.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.16</td>\n",
       "      <td>Mayhaw_Week 10.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>WRR210317XXXX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>876026</td>\n",
       "      <td>Georgia-Pacific Corporation - Cedar Springs Pa...</td>\n",
       "      <td>PPW</td>\n",
       "      <td>28.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.04</td>\n",
       "      <td>Mayhaw_Week 10.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>WRR210317XXXX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Ticket                                               Mill  \\\n",
       "0 2021-07-06  875937  Georgia-Pacific Corporation - Cedar Springs Pa...   \n",
       "1 2021-07-06  875841  Georgia-Pacific Corporation - Cedar Springs Pa...   \n",
       "2 2021-07-06  875787  Georgia-Pacific Corporation - Cedar Springs Pa...   \n",
       "3 2021-07-06  876023  Georgia-Pacific Corporation - Cedar Springs Pa...   \n",
       "4 2021-07-06  876026  Georgia-Pacific Corporation - Cedar Springs Pa...   \n",
       "\n",
       "  Product  Tons (U.S)  Cull Tons  Accepted Tons              Source Notes  \\\n",
       "0     PPW       23.27        0.0          23.27  Mayhaw_Week 10.pdf   NaN   \n",
       "1     PPW       23.86        0.0          23.86  Mayhaw_Week 10.pdf   NaN   \n",
       "2     PPW       23.99        0.0          23.99  Mayhaw_Week 10.pdf   NaN   \n",
       "3     PPW       28.16        0.0          28.16  Mayhaw_Week 10.pdf   NaN   \n",
       "4     PPW       28.04        0.0          28.04  Mayhaw_Week 10.pdf   NaN   \n",
       "\n",
       "   Reporting Period       TicketID Week  \n",
       "0                 5  WRR210317XXXX  NaN  \n",
       "1                 5  WRR210317XXXX  NaN  \n",
       "2                 5  WRR210317XXXX  NaN  \n",
       "3                 5  WRR210317XXXX  NaN  \n",
       "4                 5  WRR210317XXXX  NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mill_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the old Harvest TicketIDs\n",
    "harvested_tons_oldTID = mill_df[mill_df['Reporting Period'] == 7].groupby(\"TicketID\")['Tons (U.S)'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63343.458"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvested_tons_oldTID['Tons (U.S)'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get new and old TIDs that are currently in process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting harvest ids that were in RP 7 and are still \"Active\" (Potentially need updating due to harvest)\n",
    "harvest_activities = wrr_df[(wrr_df['Reporting Period'] == 7) & (wrr_df['Status'].isin(['IN PROCESS', \"POST\"]))]['UniqueID'].unique().tolist()\n",
    "harvest_old_ticketid = wrr_df[(wrr_df['Reporting Period'] == 7) & (wrr_df['Status'].isin(['IN PROCESS', \"POST\"]))]['TicketID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15073.17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many tons are under old TIDs of activities in question\n",
    "harvested_tons_oldTID[harvested_tons_oldTID['TicketID'].isin(harvest_old_ticketid)]['Tons (U.S)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the new TIDs based on these harvest activities\n",
    "new_ticketIDs = ticketid_table[ticketid_table['UniqueID'].isin(harvest_activities)]['TicketID'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what is in old_ticketid that is not in new_ticketIDs\n",
    "[item for item in harvest_old_ticketid if item not in new_ticketIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in harvest_old_ticketid:\n",
    "    old_unique = wrr_df[wrr_df['TicketID'] == id]['UniqueID'].unique().tolist()\n",
    "    new_unique = ticketid_table[ticketid_table['TicketID'] == id]['UniqueID'].unique().tolist()\n",
    "    inold_notinew = [item for item in old_unique if item not in new_unique]\n",
    "    if len(inold_notinew) > 0:\n",
    "        print(id, old_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so some old ticketids don't line up with the new ticket ids\n",
    "\n",
    "Let's update the mill compiled file and replace the ticket ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mill_df[mill_df['TicketID'].isin(['WRR231107CC02', 'WRR231107TT01'])]['TicketID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mill_df[mill_df['TicketID'].str.contains(\"WRR231107CC02\")]['TicketID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game plan is to get the old uniqueid -> ticketid connection and then find the new ticketid.\n",
    "# Then replace all ticketids in the mill df to the new using a map\n",
    "\n",
    "wrr_df_uid_tid = wrr_df[['UniqueID', 'TicketID']].copy()\n",
    "wrr_df_uid_tid = pd.merge(wrr_df_uid_tid, ticketid_table, on='UniqueID', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TicketID</th>\n",
       "      <th>UniqueID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WRR231107XXXX</td>\n",
       "      <td>WRR231107CC02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WRR231107XXXX</td>\n",
       "      <td>WRR231107TT01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TicketID       UniqueID\n",
       "11  WRR231107XXXX  WRR231107CC02\n",
       "12  WRR231107XXXX  WRR231107TT01"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticketid_table[ticketid_table['UniqueID'].isin(['WRR231107CC02', 'WRR231107TT01'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_df_uid_tid[wrr_df_uid_tid['TicketID_y'].str.contains('X')]\n",
    "wrr_df_tids = wrr_df_uid_tid[['TicketID_x', 'TicketID_y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_df_tids.drop_duplicates(inplace=True)\n",
    "wrr_tid_mapper = dict(zip(wrr_df_tids['TicketID_x'], wrr_df_tids['TicketID_y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df['new_ticketid'] = mill_df['TicketID'].map(wrr_tid_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257034.38800000004"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ticket_harvested = mill_df.groupby('new_ticketid')['Tons (U.S)'].sum().reset_index()\n",
    "new_ticket_harvested['Tons (U.S)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257034.388"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mill_df['Tons (U.S)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Mill</th>\n",
       "      <th>Product</th>\n",
       "      <th>Tons (U.S)</th>\n",
       "      <th>Cull Tons</th>\n",
       "      <th>Accepted Tons</th>\n",
       "      <th>Source</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Reporting Period</th>\n",
       "      <th>TicketID</th>\n",
       "      <th>Week</th>\n",
       "      <th>new_ticketid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, Ticket, Mill, Product, Tons (U.S), Cull Tons, Accepted Tons, Source, Notes, Reporting Period, TicketID, Week, new_ticketid]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mill_df[mill_df['new_ticketid'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There was an issue where WRR230220HD01 was mislabeled WRR230220CC01 in the RP7 update compiled. I fixed that at the source on 2024.11.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harvested_tons_oldTID.to_excel(\"harvested_tons_compare_old.xlsx\", sheet_name='oldTID')\n",
    "# new_ticket_harvested.to_excel(\"harvested_tons_compare_new.xlsx\", sheet_name='newTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_expected = pd.merge(ticketid_table, wrr_df[['UniqueID', 'Current RP Expected GT', 'Status', 'Future GT']], on='UniqueID', how='left')\n",
    "trial_expected = pd.merge(trial_expected, client_df[['UniqueID', 'Reporting Period']], on='UniqueID', how='left')\n",
    "wrr_rp7_expected = trial_expected[(trial_expected['Reporting Period'] == 7) & trial_expected['Status'].isin(['IN PROCESS', 'OPEN', 'POST'])].groupby('TicketID')['Current RP Expected GT'].sum().reset_index()\n",
    "wrr_rp7_expected = pd.merge(wrr_rp7_expected, harvested_tons_oldTID, on='TicketID', how='left')\n",
    "wrr_rp7_expected['Tons (U.S)'] = pd.to_numeric(wrr_rp7_expected['Tons (U.S)'], errors='coerce', downcast='float')\n",
    "wrr_rp7_expected['Current RP Expected GT'] = pd.to_numeric(wrr_rp7_expected['Current RP Expected GT'], errors='coerce', downcast='float')\n",
    "wrr_rp7_expected['Tons (U.S)'].fillna(0, inplace=True)\n",
    "wrr_rp7_expected['Current RP Expected GT'].fillna(0, inplace=True)\n",
    "wrr_rp7_expected['Remaining'] = wrr_rp7_expected['Current RP Expected GT'] - wrr_rp7_expected['Tons (U.S)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TicketID</th>\n",
       "      <th>Current RP Expected GT</th>\n",
       "      <th>Tons (U.S)</th>\n",
       "      <th>Remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WRR230829CC04</td>\n",
       "      <td>252.679993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>252.679993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WRR230829CCXX</td>\n",
       "      <td>8077.149902</td>\n",
       "      <td>5586.919922</td>\n",
       "      <td>2490.229980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WRR231107CC01</td>\n",
       "      <td>891.099976</td>\n",
       "      <td>700.049988</td>\n",
       "      <td>191.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WRR231107CC03</td>\n",
       "      <td>903.630005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>903.630005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WRR231107TH01</td>\n",
       "      <td>282.130005</td>\n",
       "      <td>215.100006</td>\n",
       "      <td>67.029999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WRR231107XXXX</td>\n",
       "      <td>10987.530273</td>\n",
       "      <td>4989.770020</td>\n",
       "      <td>5997.760254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WRR240101TH01</td>\n",
       "      <td>541.820007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>541.820007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WRR240101TH02</td>\n",
       "      <td>888.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>888.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WRR240101TT01</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999.900024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WRR240101XXXX</td>\n",
       "      <td>12298.480469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12298.480469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WRR240404TH01</td>\n",
       "      <td>920.400024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>920.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WRR240404TH02</td>\n",
       "      <td>2090.399902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2090.399902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WRR240517TH01</td>\n",
       "      <td>5916.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5916.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TicketID  Current RP Expected GT   Tons (U.S)     Remaining\n",
       "0   WRR230829CC04              252.679993     0.000000    252.679993\n",
       "1   WRR230829CCXX             8077.149902  5586.919922   2490.229980\n",
       "4   WRR231107CC01              891.099976   700.049988    191.049988\n",
       "5   WRR231107CC03              903.630005     0.000000    903.630005\n",
       "6   WRR231107TH01              282.130005   215.100006     67.029999\n",
       "7   WRR231107XXXX            10987.530273  4989.770020   5997.760254\n",
       "9   WRR240101TH01              541.820007     0.000000    541.820007\n",
       "10  WRR240101TH02              888.250000     0.000000    888.250000\n",
       "11  WRR240101TT01              999.900024     0.000000    999.900024\n",
       "12  WRR240101XXXX            12298.480469     0.000000  12298.480469\n",
       "13  WRR240404TH01              920.400024     0.000000    920.400024\n",
       "14  WRR240404TH02             2090.399902     0.000000   2090.399902\n",
       "15  WRR240517TH01             5916.000000     0.000000   5916.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrr_rp7_expected[wrr_rp7_expected['Remaining'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrr_rp7_expected.to_excel(\"wrr_rp7_accouting.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_expected = trial_expected[(trial_expected['Reporting Period'] == 8) & (trial_expected['Status'].isin(['OPEN', 'IN PROCESS', 'PENDING', 'POST']))].groupby('TicketID')['Current RP Expected GT'].sum().reset_index()\n",
    "wrr_rp8_expected_1 = trial_expected[(trial_expected['Reporting Period'] == 7) & (trial_expected['Status'].isin(['OPEN', 'IN PROCESS', 'PENDING', 'POST']))].groupby('TicketID')['Future GT'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_expected = pd.merge(wrr_rp8_expected, wrr_rp8_expected_1, on='TicketID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_expected['Current RP Expected GT'] = pd.to_numeric(wrr_rp8_expected['Current RP Expected GT'], errors='coerce', downcast='float')\n",
    "wrr_rp8_expected['Future GT'] = pd.to_numeric(wrr_rp8_expected['Future GT'], errors='coerce', downcast='float')\n",
    "wrr_rp8_expected.fillna(0, inplace=True)\n",
    "wrr_rp8_expected['RP8_Expected'] = wrr_rp8_expected['Current RP Expected GT'] + wrr_rp8_expected['Future GT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_trial = pd.merge(wrr_rp8_expected, wrr_rp7_expected, on='TicketID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_rp8_trial\n",
    "wrr_rp8_trial.fillna(0, inplace=True)\n",
    "wrr_rp8_trial['final_expected'] = wrr_rp8_trial['RP8_Expected'] + wrr_rp8_trial['Remaining']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, this adds up with Rob's file. Lets get the info needed to fill out the Shared Space table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_ticketid_volume = wrr_rp8_trial[['TicketID', 'final_expected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_mapper = dict(zip(ticketid_table['UniqueID'], ticketid_table['TicketID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_df['new_ticket_id'] = wrr_df['UniqueID'].map(ticket_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrr_rp8_trial.to_excel('wrr_rp8_investigate_3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TicketID</th>\n",
       "      <th>UniqueID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WRR231107XXXX</td>\n",
       "      <td>WRR231107CC02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WRR231107XXXX</td>\n",
       "      <td>WRR231107TT01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TicketID       UniqueID\n",
       "11  WRR231107XXXX  WRR231107CC02\n",
       "12  WRR231107XXXX  WRR231107TT01"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticketid_table.loc[ticketid_table['TicketID'] == 'WRR231107XXXX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok. Let's create the client facing table and get it up on the drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>Tract</th>\n",
       "      <th>CMP</th>\n",
       "      <th>Stand</th>\n",
       "      <th>Acres</th>\n",
       "      <th>Age</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>Stand Type</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Scheduled</th>\n",
       "      <th>PreBA</th>\n",
       "      <th>PreTPA</th>\n",
       "      <th>PreGTA</th>\n",
       "      <th>PlnBA</th>\n",
       "      <th>PlnTPA</th>\n",
       "      <th>PlnGTA</th>\n",
       "      <th>Client GT</th>\n",
       "      <th>Future GT</th>\n",
       "      <th>Reporting Period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>WRR240919TT04</td>\n",
       "      <td>Mayhaw</td>\n",
       "      <td>122-1</td>\n",
       "      <td>10</td>\n",
       "      <td>19.14</td>\n",
       "      <td>29</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>GA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>180</td>\n",
       "      <td>30</td>\n",
       "      <td>574.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>WRR240919TT03</td>\n",
       "      <td>Mayhaw</td>\n",
       "      <td>122-1</td>\n",
       "      <td>9</td>\n",
       "      <td>14.55</td>\n",
       "      <td>42</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>GA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>115</td>\n",
       "      <td>175</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>35</td>\n",
       "      <td>363.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>WRR240919TT02</td>\n",
       "      <td>Mayhaw</td>\n",
       "      <td>122-1</td>\n",
       "      <td>9</td>\n",
       "      <td>41.84</td>\n",
       "      <td>42</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>GA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>115</td>\n",
       "      <td>175</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>35</td>\n",
       "      <td>1046.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>WRR240919TT01</td>\n",
       "      <td>Mayhaw</td>\n",
       "      <td>122-1</td>\n",
       "      <td>8</td>\n",
       "      <td>16.27</td>\n",
       "      <td>44</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>GA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>130</td>\n",
       "      <td>190</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>35</td>\n",
       "      <td>569.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>WRR240919TH01</td>\n",
       "      <td>Mayhaw</td>\n",
       "      <td>122-1</td>\n",
       "      <td>6</td>\n",
       "      <td>15.32</td>\n",
       "      <td>28</td>\n",
       "      <td>Early</td>\n",
       "      <td>GA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>125</td>\n",
       "      <td>350</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>413.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         UniqueID   Tract    CMP Stand  Acres  Age County State  Stand Type  \\\n",
       "43  WRR240919TT04  Mayhaw  122-1    10  19.14   29  EARLY    GA           1   \n",
       "44  WRR240919TT03  Mayhaw  122-1     9  14.55   42  EARLY    GA           0   \n",
       "45  WRR240919TT02  Mayhaw  122-1     9  41.84   42  EARLY    GA           0   \n",
       "46  WRR240919TT01  Mayhaw  122-1     8  16.27   44  EARLY    GA           0   \n",
       "47  WRR240919TH01  Mayhaw  122-1     6  15.32   28  Early    GA           1   \n",
       "\n",
       "    Origin Scheduled  PreBA  PreTPA  PreGTA  PlnBA  PlnTPA  PlnGTA  Client GT  \\\n",
       "43       0              130     250      60     70     180      30     574.20   \n",
       "44       0              115     175      60     70      95      35     363.75   \n",
       "45       0              115     175      60     70      95      35    1046.00   \n",
       "46       0              130     190      70     70      95      35     569.45   \n",
       "47       0              125     350      67     70     200      40     413.64   \n",
       "\n",
       "   Future GT  Reporting Period  \n",
       "43       0.0                 8  \n",
       "44       0.0                 8  \n",
       "45       0.0                 8  \n",
       "46       0.0                 8  \n",
       "47       0.0                 8  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_intake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df.head() # make sure to use the correct ticketid long term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tcs_intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_inprog[['UniqueID', 'TCS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog = status_df[status_df['Status'].isin(['OPEN', 'IN PROCESS', 'POST'])].copy()\n",
    "client_table_inprog = pd.merge(client_table_inprog, wrr_inprog, on='UniqueID', how='left')\n",
    "client_table_inprog = pd.merge(client_table_inprog, ticketid_table, on='UniqueID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog = client_table_inprog[['UniqueID', 'TCS', 'Status_x', 'Guidance_x', 'TicketID_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_aligned_with_rob = pd.read_excel(\"./WRR_RP8_InProcess/wrr_rp8_investigate_3.xlsx\")\n",
    "expected_aligned_with_rob = expected_aligned_with_rob[['TicketID', 'Final Values']]\n",
    "expected_aligned_with_rob['RP8_Expected'] = pd.to_numeric(expected_aligned_with_rob['Final Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog = pd.merge(client_table_inprog, expected_aligned_with_rob, left_on='TicketID_y', right_on='TicketID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_table_inprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_final_df = client_table_inprog.groupby('TicketID').agg({'TCS': \", \".join,\n",
    "                                               'Status_x': \", \".join,\n",
    "                                               'Guidance_x': \", \".join,\n",
    "                                               'RP8_Expected': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_final_df['RP8_Expected'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I totally did not make the expected by RP table\n",
    "\n",
    "I need to get total expected by activity\n",
    "\n",
    "and\n",
    "\n",
    "I need to get the expected by RP\n",
    "\n",
    "Fortunately WRR doesn't have any \"Future\" rp volumes to worry about - just carry over, and we are only worrying about RP7 -> RP8 for that\n",
    "\n",
    "and RP8 is figured out already from above.\n",
    "\n",
    "So, we just need to grab all the previous volumes to build out the individual RP expected volumes.\n",
    "\n",
    "The trick will be finding the total? Or should we just base it off the total in mss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_df[['UniqueID', 'Reporting Period', 'Current RP Expected GT', 'Total GT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog = wrr_df[wrr_df['Reporting Period'].notna()][['UniqueID', 'Reporting Period', 'Current RP Expected GT', 'Total GT']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog['Reporting Period'].fillna(0, inplace=True)\n",
    "wrr_expected_inprog.loc[client_df['Reporting Period'] == \"\", 'Reporting Period'] = 0\n",
    "s = wrr_expected_inprog.loc[pd.to_numeric(wrr_expected_inprog['Reporting Period'], errors='coerce').isnull() & wrr_expected_inprog['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'].apply(lambda x: x.strip()[-1])\n",
    "wrr_expected_inprog['trial'] = wrr_expected_inprog.index.map(s)\n",
    "wrr_expected_inprog.loc[pd.to_numeric(wrr_expected_inprog['Reporting Period'], errors='coerce').isnull() & wrr_expected_inprog['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'] = wrr_expected_inprog.loc[pd.to_numeric(wrr_expected_inprog['Reporting Period'], errors='coerce').isnull() & wrr_expected_inprog['Reporting Period'].str.contains(\"RP\"), 'trial']\n",
    "wrr_expected_inprog['Reporting Period'] = pd.to_numeric(wrr_expected_inprog['Reporting Period'], downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog['Current RP Expected GT'] = pd.to_numeric(wrr_expected_inprog['Current RP Expected GT'], errors='coerce')\n",
    "wrr_expected_inprog['Total GT'] = pd.to_numeric(wrr_expected_inprog['Total GT'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog = wrr_expected_inprog[((wrr_expected_inprog['Current RP Expected GT'].notna()) | (wrr_expected_inprog['Total GT'].notna())) & (wrr_expected_inprog['Reporting Period'] < 8)]\n",
    "wrr_expected_inprog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, well.. Looks like it is all RP7 - but that is the data we have.\n",
    "\n",
    "Time to tie in the TicketID and then groupby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected_inprog = pd.merge(wrr_expected_inprog, ticketid_table, on='UniqueID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected = wrr_expected_inprog.groupby(['TicketID', 'Reporting Period'])['Current RP Expected GT'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp8_expect = client_final_df[['TicketID', 'RP8_Expected']].copy()\n",
    "rp8_expect['Reporting Period'] = 8\n",
    "rp8_expect['Current RP Expected GT'] = rp8_expect['RP8_Expected']\n",
    "rp8_expect = rp8_expect[['TicketID', 'Current RP Expected GT', 'Reporting Period']].copy()\n",
    "wrr_expected =pd.concat([wrr_expected, rp8_expect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the aggregation of \"total gt\" for each new ticketid? \n",
    "\n",
    "After some looking - it looks like we don't need to worry about this for the current rp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_expect_inprog = wrr_df[['UniqueID', 'Total GT']].copy()\n",
    "total_expect_inprog = pd.merge(total_expect_inprog, ticketid_table, on='UniqueID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_expect_inprog['Total GT'] = pd.to_numeric(total_expect_inprog['Total GT'], errors='coerce')\n",
    "total_expect_interest = total_expect_inprog[total_expect_inprog['Total GT'] > 0].copy()\n",
    "ticket_total = total_expect_interest.groupby('TicketID')['Total GT'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_total.to_excel('first_ticket_total.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the harvested volumes for RP8 and total then for these magical beasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mill_rp = mill_df.groupby(['new_ticketid', 'Reporting Period'])['Tons (U.S)'].sum().reset_index()\n",
    "grouped_mill_total = mill_df.groupby('new_ticketid')['Tons (U.S)'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# able to get either at rp level or total for ticket id\n",
    "# new_ticket_total = pd.merge(ticket_total, grouped_mill_rp[grouped_mill_rp['Reporting Period'] > 7], left_on='TicketID', right_on='new_ticketid', how='inner')\n",
    "new_ticket_total = pd.merge(ticket_total, grouped_mill_total, left_on='TicketID', right_on='new_ticketid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_ticket_total.to_excel('total_volume.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the last ticket date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_recentdate = mill_df.groupby('new_ticketid')['Date'].last().reset_index()\n",
    "# ticket_recentdate.to_excel('ticketdate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['UniqueID'] == 'WRR240101CC01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['TicketID'].str.contains('X')]['TicketID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['TicketID'] == 'WRR231107XXXX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_df[mill_df['TicketID'] == 'WRR231107CC02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lst = ticketid_table['UniqueID'].unique().tolist()\n",
    "len(wrr_df[(wrr_df['UniqueID'].isin(unique_lst)) & (wrr_df['Status'].isin(['IN PROCESS', 'OPEN', 'POST']))]['TicketID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inprog_wrr_df = wrr_df[wrr_df['Status'].isin(['IN PROCESS', 'POST'])].copy()\n",
    "inprog_wrr_oldTID = inprog_wrr_df.groupby(['TicketID'])['UniqueID'].apply(list).reset_index()\n",
    "# inprog_wrr_oldTID.to_excel(\"old_tid_unique.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_table[ticketid_table['TicketID'].isin(['WRR230829CCXX',\n",
    "                                                'WRR230829TH01',\n",
    "                                                'WRR230829TH02',\n",
    "                                                'WRR231107CC01',\n",
    "                                                'WRR231107CC03',\n",
    "                                                'WRR231107TH01',\n",
    "                                                'WRR231107XXXX',\n",
    "                                                'WRR240101TH01',\n",
    "                                                'WRR240101TT01',\n",
    "                                                'WRR240101XXXX',\n",
    "                                                'WRR240404TH01',\n",
    "                                                'WRR240404TH02',\n",
    "                                                'WRR240709CCXX',\n",
    "                                                'WRR240802TH01',\n",
    "                                                'WRR240802TH03'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgres_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
