{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, MetaData, text, update\n",
    "from smartsheet_dataframe import get_as_df\n",
    "import sqlite3\n",
    "from sqlalchemy.dialects.sqlite import insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_on_conflict_nothing(table, conn, keys, data_iter):\n",
    "    # \"a\" is the primary key in the \"conflict_table\"\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "    stmt = insert(table.table).values(data).on_conflict_do_nothing(index_elements=[\"uniqueid\"])\n",
    "    result = conn.execute(stmt)\n",
    "    return result.rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_pre_imp = pd.read_excel(\"./data/bjt_trial_tables.xlsx\", sheet_name='client_pre_imp')\n",
    "ga_pre_imp = pd.read_excel(\"./data/bjt_trial_tables.xlsx\", sheet_name='ga_pre_imp')\n",
    "pre_imp_plot = pd.read_excel(\"./data/bjt_trial_tables.xlsx\", sheet_name='pre_imp_plot')\n",
    "expected_harvest = pd.read_excel(\"./data/bjt_trial_tables.xlsx\", sheet_name='expected_harvest')\n",
    "ticket_id = pd.read_excel(\"./data/bjt_trial_tables.xlsx\", sheet_name='ticket_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column fixing bits\n",
    "# Fix data typing issues\n",
    "ga_pre_imp['guidance_expiration'] = ga_pre_imp['guidance_expiration'].dt.date\n",
    "ga_pre_imp['guidance_presented'] = ga_pre_imp['guidance_presented'].dt.date\n",
    "\n",
    "# Fix naming issues\n",
    "client_pre_imp.rename(columns={\"cmp\": \"comp\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your connection to the sqlite db - if it doesn't exist it gets created\n",
    "proj_code = \"P:\\\\Temp\\\\harvest_db_trial\\\\harvest_db_trial\"\n",
    "conn = sqlite3.connect(f'{proj_code}.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out adding in any new data from client_preimp\n",
    "# find existing keys\n",
    "client_pre_imp_existing_ids = pd.read_sql_query(f'SELECT uniqueid FROM client_preimp', conn)\n",
    "# existing keys in both data sets\n",
    "new_rows = client_pre_imp[~client_pre_imp['uniqueid'].isin(client_pre_imp_existing_ids['uniqueid'])]\n",
    "# add in new data to the database else do nothing\n",
    "if not new_rows.empty:\n",
    "    new_rows.to_sql('client_preimp', conn, if_exists='append', index=False)\n",
    "del new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try adding ga_pre_imp data to the db\n",
    "# get uniqueids already in db table\n",
    "ga_pre_imp_existing_ids = pd.read_sql_query(f'SELECT uniqueid FROM ga_preimp', conn)\n",
    "# get the rows that are in the excel file but not in the db table\n",
    "new_rows = ga_pre_imp[~ga_pre_imp['uniqueid'].isin(ga_pre_imp_existing_ids['uniqueid'])]\n",
    "# add in the new data\n",
    "if not new_rows.empty:\n",
    "    new_rows.to_sql('ga_preimp', conn, if_exists='append', index=False)\n",
    "del new_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategize\n",
    "\n",
    "Ok, I think there are a number of ways that we can streamline this process.\n",
    "\n",
    "1. map out each table's unique id and set up a function that does the above addition of \"new_rows\"\n",
    "2. figure out how to prevent duplication of data in ticketid table and plotid table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5\n",
    "ref_dict = {\"client_preimp\": [client_pre_imp, 'uniqueid'],\n",
    "            \"ga_preimp\": [ga_pre_imp, \"uniqueid\"],\n",
    "            \"expected_harvest\": [expected_harvest, \"ticketid\"],\n",
    "            \"preimp_plot\": [pre_imp_plot, ['uniqueid', 'plotid']],\n",
    "            \"ticketid\": [ticket_id, ['uniqueid', 'ticketid']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \n",
    "def add_new_rows(df, table, conn, id):\n",
    "    if len(id) == 2:\n",
    "        try:\n",
    "            existing_ids = pd.read_sql_query(f'SELECT {id[0]}, {id[1]} from {table}', conn)\n",
    "            merged = df.merge(existing_ids, how='left', indicator=True)\n",
    "            new_rows = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        except:\n",
    "            print(f\"Something went wrong with adding new rows to {table}\")\n",
    "    else:\n",
    "        try:    \n",
    "            existing_ids = pd.read_sql_query(f'SELECT {id} from {table}', conn)\n",
    "            new_rows = df[~df[f'{id}'].isin(existing_ids[f'{id}'])]\n",
    "        except:\n",
    "            print(f\"Something went wrong with adding new rows to {table}\")    \n",
    "    \n",
    "    \n",
    "    if not new_rows.empty:\n",
    "        new_rows.to_sql(table, conn, if_exists='append', index=False)\n",
    "    print(table, len(new_rows))\n",
    "    del new_rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "client_preimp 0\n",
      "1\n",
      "ga_preimp 0\n",
      "2\n",
      "expected_harvest 0\n",
      "3\n",
      "preimp_plot 0\n",
      "4\n",
      "ticketid 0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key in ref_dict.keys():\n",
    "    print(i)\n",
    "    add_new_rows(ref_dict[key][0], key, conn, ref_dict[key][1])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugger = {\"numbers\": [1, 2, 3, 4, 5, 6],\n",
    "          \"letters\": [\"a\", \"b\", \"c\", \"d\", \"e\", 'f']\n",
    "}\n",
    "bugger1 = {\"numbers\": [1, 2, 7, 8, 9, 10],\n",
    "          \"letters\": [\"a\", \"b\", \"g\", \"h\", \"i\", 'j']\n",
    "}\n",
    "\n",
    "dfa =pd.DataFrame(bugger)\n",
    "dfb = pd.DataFrame(bugger1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranger = [i for i in range(len(dfa))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgres_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
