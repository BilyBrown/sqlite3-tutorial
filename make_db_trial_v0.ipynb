{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smartsheet_dataframe import get_as_df\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Smartsheet information in variables\n",
    "shitz_token = \"wMvGniITjDFd7ClZPE44wtJGvZkM7Hg7mP7if\" #Bily's personal token to access G-A smartsheet data. Anyone can obtain their own token\n",
    "fhaid = 4523924143794052 # The ID number that references the FHA sheet\n",
    "mssid = 3005704744265604 # The ID number that references the MSS sheet\n",
    "\n",
    "# Creating the FHA and MSS dataframes\n",
    "fha_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=fhaid)\n",
    "\n",
    "mss_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=mssid)\n",
    "\n",
    "data_df = mss_df.loc[(mss_df['UniqueID'].str.contains('WAR')) |\n",
    "                     (mss_df['UniqueID'].str.contains('WRR')) |\n",
    "                     (mss_df['UniqueID'].str.contains('BJT'))].copy()\n",
    "\n",
    "data_df['Tract'] = data_df['Tract'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "data_df['CMP'] = data_df['CMP'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "data_df['Stand'] = data_df['Stand'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "data_df['TCS'] = data_df['Tract'] + \"_\" + data_df['CMP'] + \"_\" + data_df['Stand']\n",
    "\n",
    "data_df['Reporting Period'] = data_df['Reporting Period'].fillna(0)\n",
    "data_df.loc[data_df['Reporting Period'] == \"\", 'Reporting Period'] = 0\n",
    "s = data_df.loc[pd.to_numeric(data_df['Reporting Period'], errors='coerce').isnull() & data_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'].apply(lambda x: x.strip()[-1])\n",
    "data_df['trial'] = data_df.index.map(s)\n",
    "data_df.loc[pd.to_numeric(data_df['Reporting Period'], errors='coerce').isnull() & data_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'] = data_df.loc[pd.to_numeric(data_df['Reporting Period'], errors='coerce').isnull() & data_df['Reporting Period'].str.contains(\"RP\"), 'trial']\n",
    "data_df['Reporting Period'] = pd.to_numeric(data_df['Reporting Period'], downcast='integer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaet Client Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the Client Provided data and make sure the fields are similar\n",
    "client_df = data_df[['UniqueID',\n",
    "                    'Tract',\n",
    "                    'CMP',\n",
    "                    'Stand',\n",
    "                    'Acres',\n",
    "                    'Age',\n",
    "                    'County',\n",
    "                    'State',\n",
    "                    'Stand Type',\n",
    "                    'Origin',\n",
    "                    'Scheduled',\n",
    "                    'PreBA',\n",
    "                    'PreTPA',\n",
    "                    'PreGTA',\n",
    "                    'PlnBA',\n",
    "                    'PlnTPA',\n",
    "                    'PlnGTA',\n",
    "                    'Client GT',\n",
    "                    'Future GT',\n",
    "                    'Reporting Period']].copy()\n",
    "\n",
    "client_df['Age'] = pd.to_numeric(client_df['Age'], errors='coerce', downcast='integer').fillna(0).astype(int)\n",
    "client_df.loc[pd.to_numeric(client_df['Future GT'], errors='coerce').isnull(), 'Future GT'] = 0\n",
    "client_df['Tract'] = client_df['Tract'].astype(str)\n",
    "client_df['Stand'] = client_df['Stand'].astype(str)\n",
    "client_df['Tract'] = client_df['Tract'].apply(lambda x: x.split('.')[0])\n",
    "client_df['Client GT'] = pd.to_numeric(client_df['Client GT'], errors=\"coerce\")\n",
    "client_df['Client GT'] = client_df['Client GT'].fillna(0)\n",
    "\n",
    "cols = ['PreBA', 'PreTPA', 'PreGTA', 'PlnBA', 'PlnTPA', 'PlnGTA']\n",
    "\n",
    "for x in cols:\n",
    "    client_df[x] = pd.to_numeric(client_df[x], errors='coerce', downcast='integer').fillna(-99).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Normalize some of the fields in the client intake table\n",
    "\n",
    "## Normalize Species\n",
    "species = [\"Slash Pine\", \"Loblolly Pine\", 'Mixed Natural Pine', 'Hardwood Mix', 'Pine Straw', \"Other\"]\n",
    "species_dict = {'Species': species}\n",
    "species_df = pd.DataFrame(species_dict)\n",
    "species_df['SpeciesUID'] = species_df.index\n",
    "\n",
    "def species_map(x):\n",
    "    x = x.lower()\n",
    "    if \"slash\" in x:\n",
    "        return 0\n",
    "    elif \"loblol\" in x:\n",
    "        return 1\n",
    "    elif \"mixed natural\" in x:\n",
    "        return 2\n",
    "    elif \"h\" in x:\n",
    "        return 3\n",
    "    elif \"straw\" in x:\n",
    "        return 4\n",
    "    elif \"pine\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "client_df['Stand Type'] = client_df['Stand Type'].apply(lambda x: species_map(x))\n",
    "\n",
    "\n",
    "## Normalize origin\n",
    "origins = ['Planted', 'Natural', 'Natural Mix', 'Other']\n",
    "origin_dict = {'Origin': origins}\n",
    "origin_df = pd.DataFrame(origin_dict)\n",
    "origin_df['OriginUID'] = origin_df.index\n",
    "\n",
    "def origin_map(x):\n",
    "    x = x.lower()\n",
    "    if \"plant\" in x:\n",
    "        return 0\n",
    "    elif \"natural mix\" in x:\n",
    "        return 2\n",
    "    elif \"nat\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 3\n",
    "client_df['Origin'] = client_df['Origin'].apply(lambda x: origin_map(x))\n",
    "\n",
    "\n",
    "## Normalize tract\n",
    "client_df['tract_l'] = client_df['Tract'].apply(lambda x: x.lower())\n",
    "tracts = client_df['tract_l'].unique().tolist()\n",
    "tract_dict = {'tract': tracts}\n",
    "tract_df = pd.DataFrame(tract_dict)\n",
    "tract_df['tract_oid'] = tract_df.index\n",
    "new_tract_dict = dict(zip(tract_df['tract'], tract_df['tract_oid']))\n",
    "client_df['Tract'] = client_df['tract_l'].map(new_tract_dict)\n",
    "\n",
    "## Normalize county\n",
    "client_df['County'] = client_df['County'].astype(str)\n",
    "client_df['county_l'] = client_df['County'].apply(lambda x: x.lower())\n",
    "countys = client_df['county_l'].unique().tolist()\n",
    "county_dict = {'county': countys}\n",
    "county_df = pd.DataFrame(county_dict)\n",
    "county_df['county_oid'] = county_df.index\n",
    "new_county_dict = dict(zip(county_df['county'], county_df['county_oid']))\n",
    "client_df['County'] = client_df['county_l'].map(new_county_dict)\n",
    "\n",
    "## Normalize state\n",
    "client_df['State'] = client_df['State'].astype(str)\n",
    "client_df['state_l'] = client_df['State'].apply(lambda x: x.lower())\n",
    "states = client_df['state_l'].unique().tolist()\n",
    "state_dict = {'state': states}\n",
    "state_df = pd.DataFrame(state_dict)\n",
    "state_df['state_oid'] = state_df.index\n",
    "new_state_dict = dict(zip(state_df['state'], state_df['state_oid']))\n",
    "client_df['State'] = client_df['state_l'].map(new_state_dict)\n",
    "\n",
    "# Drop the lowercase columns\n",
    "client_df.drop(columns=['tract_l', 'county_l', 'state_l'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GA Intake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_intake_df = data_df[['IntakeID',\n",
    "                       'FolderID',\n",
    "                       'UniqueID',\n",
    "                       'GA_ACRES',\n",
    "                       'GA GT', # this is going to be GA total isn't it? Or does it stay as GA estimate of current GT (total minus what client says is future)\n",
    "                       'Total GT', # This will be the conservative total between client and GA numbers\n",
    "                       'Notes'\n",
    "                       ]].copy()\n",
    "\n",
    "# Lowercase the ga_intake_df column names\n",
    "ga_intake_df.columns = [x.lower().replace(' ', '_') for x in ga_intake_df.columns]\n",
    "ga_intake_df.rename(columns={'uniqueid': 'id'},inplace=True)\n",
    "ga_intake_df = ga_intake_df[['id', 'intakeid', 'folderid', 'ga_acres', 'ga_gt', 'total_gt', 'notes']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Status Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = data_df[['UniqueID',\n",
    "                    'Guidance',\n",
    "                    'Expiration',\n",
    "                    'GuidanceProvider',\n",
    "                    'Status',\n",
    "                    'Completed Date'\n",
    "                    ]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TicketID -> UniqueID lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_lookup_df = data_df[['UniqueID', 'TicketID']].copy() # the uniqueid will the the unique id for this table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_list = data_df['TicketID'].unique().tolist()\n",
    "ticketid_dict = {'ticketid': ticketid_list}\n",
    "ticketid_df = pd.DataFrame(ticketid_dict)\n",
    "ticketid_df['ticketid_oid'] = ticketid_df.index\n",
    "new_ticketid_dict = dict(zip(ticketid_df['ticketid'], ticketid_df['ticketid_oid']))\n",
    "ticketid_lookup_df['ticketid'] = ticketid_lookup_df['TicketID'].map(new_ticketid_dict)\n",
    "\n",
    "ticketid_lookup_df.rename(columns={'TicketUID': 'id',\n",
    "                                    'UniqueID': 'activity_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the TCS lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_df = data_df.groupby('TicketID').agg({'Tract': 'first',\n",
    "                                               'CMP': 'first',\n",
    "                                               'Stand': ', '.join}).reset_index()\n",
    "\n",
    "def split_stands(x):\n",
    "    bugger_splot = list(set(x.split(', ')))\n",
    "    bugger_splot.sort()\n",
    "    return \", \".join(bugger_splot)\n",
    "    \n",
    "tcs_df['stands'] = tcs_df['Stand'].apply(split_stands)\n",
    "tcs_df['comp'] = tcs_df['CMP'].apply(lambda x: x if x!=\"NP\" else \"\")\n",
    "tcs_df['TCS'] = tcs_df['Tract'] + \"_\" + tcs_df['comp'] + \"_\" + tcs_df['stands']\n",
    "tcs_df['TCS'] = tcs_df['TCS'].apply(lambda x: x.replace(\"__\", \"_\"))\n",
    "tcs_df['tcs_oid'] = tcs_df.index\n",
    "tcs_df = tcs_df[['tcs_oid', 'TicketID', 'TCS']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicketID table should have a link to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_mapper = dict(zip(tcs_df['TicketID'], tcs_df['tcs_oid']))\n",
    "ticketid_df['tcs'] = ticketid_df['ticketid'].map(tcs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_df[ticketid_df['tcs'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place the above tables into the Database\n",
    "- [X] client_df\n",
    "- [X] ga_intake_df\n",
    "- [X] origin_df\n",
    "- [X] county_df\n",
    "- [X] species_df\n",
    "- [X] state_df\n",
    "- [X] status_df\n",
    "- [X] tcs_df\n",
    "- [X] ticketid_df\n",
    "- [X] ticketid_lookup_df\n",
    "- [X] tract_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase all of the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client table\n",
    "## lowercase client_df field names\n",
    "client_df.columns = [x.lower() for x in client_df.columns]\n",
    "client_df.columns = [x.replace(' ', '_') for x in client_df.columns]\n",
    "client_df['id'] = client_df['uniqueid']\n",
    "client_df.drop(columns=['uniqueid'],inplace=True)\n",
    "client_df = client_df[['id', 'tract', 'cmp', 'stand', 'acres', 'age', 'county', 'state', 'stand_type', 'origin', 'scheduled', 'preba', 'pretpa', 'pregta',\n",
    "                       'plnba', 'plntpa', 'plngta', 'client_gt', 'future_gt', 'reporting_period']]\n",
    "\n",
    "# ga_intake\n",
    "ga_intake_df.columns = [x.lower().replace(' ', '_') for x in ga_intake_df.columns]\n",
    "ga_intake_df.rename(columns={'uniqueid': 'id'},inplace=True)\n",
    "ga_intake_df = ga_intake_df[['id', 'intakeid', 'folderid', 'ga_acres', 'ga_gt', 'total_gt']]\n",
    "\n",
    "# origin_df\n",
    "origin_df.rename(columns={'OriginUID': 'id',\n",
    "                          'Origin': 'origin'}, inplace=True)\n",
    "\n",
    "# county_df\n",
    "county_df.columns = [x.lower() for x in county_df.columns]\n",
    "\n",
    "# species_df\n",
    "species_df.columns = [x.lower() for x in species_df.columns]\n",
    "\n",
    "# state_df\n",
    "state_df.columns = [x.lower() for x in state_df.columns]\n",
    "\n",
    "# status_df\n",
    "status_df.columns = [x.lower() for x in status_df.columns]\n",
    "\n",
    "# tcs_df\n",
    "tcs_df.columns = [x.lower() for x in tcs_df.columns]\n",
    "\n",
    "# ticketid_df\n",
    "ticketid_df.columns = [x.lower() for x in ticketid_df.columns]\n",
    "\n",
    "# ticketid_lookup_df\n",
    "ticketid_lookup_df.columns = [x.lower() for x in ticketid_lookup_df.columns]\n",
    "\n",
    "# tract_df\n",
    "tract_df.columns = [x.lower() for x in tract_df.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mill Ticket sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projs = ['WAR', 'WRR', 'BJT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_loc = r\"P:\\Mill Tickets\"\n",
    "mega_df = pd.DataFrame()\n",
    "\n",
    "for proj in projs:\n",
    "    file_pattern = f\"{proj}_RP*_Compiled.xlsm\"\n",
    "    big_df = pd.DataFrame()\n",
    "\n",
    "    mill_name = []\n",
    "    for root, dirs, files in os.walk(mill_loc):\n",
    "        for file in files:\n",
    "            if f\"{proj}_RP\" in file and file.endswith(\"Compiled.xlsm\"):\n",
    "                if \"$\" in file:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(root)\n",
    "                    print(file)\n",
    "                    df = pd.read_excel(os.path.join(root, file), sheet_name='RP_Project')\n",
    "                    big_df = pd.concat([big_df, df])\n",
    "    mega_df = pd.concat([mega_df, big_df])\n",
    "                    \n",
    "'''\n",
    "    big_df.loc[big_df['Tons (U.S)'].isnull(), 'Tons (U.S)'] = big_df.loc[big_df['Tons (U.S)'].isnull(), 'TonsUS']\n",
    "    big_df['Cull Tons'].fillna(big_df['CullTons'], inplace=True)\n",
    "    big_df['Accepted Tons'].fillna(big_df['AcceptedTons'], inplace=True)\n",
    "    big_df['Reporting Period'].fillna(big_df['ReportingPeriod'],inplace=True)\n",
    "\n",
    "    def rep_fix(x):\n",
    "        if \"RP\" in str(x):\n",
    "            return x[-1]\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    big_df['Reporting Period'] = big_df['Reporting Period'].apply(lambda x: rep_fix(x))\n",
    "    big_df['Reporting Period'] = pd.to_numeric(big_df['Reporting Period'])\n",
    "\n",
    "    big_df['Date'] = pd.to_datetime(big_df['Date'])\n",
    "\n",
    "    harvest_df = big_df[['Date',\n",
    "                    'Ticket',\n",
    "                    'Mill',\n",
    "                    'Product',\n",
    "                    'Tons (U.S)',\n",
    "                    'Cull Tons',\n",
    "                    'Accepted Tons',\n",
    "                    'Source',\n",
    "                    'Notes',\n",
    "                    'Reporting Period',\n",
    "                    'TicketID',\n",
    "                    'Week']].copy()\n",
    "\n",
    "    harvest_df = harvest_df.drop_duplicates()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_df.loc[mega_df['Tons (U.S)'].isnull(), 'Tons (U.S)'] = mega_df.loc[mega_df['Tons (U.S)'].isnull(), 'TonsUS']\n",
    "mega_df['Cull Tons'].fillna(mega_df['CullTons'], inplace=True)\n",
    "mega_df['Accepted Tons'].fillna(mega_df['AcceptedTons'], inplace=True)\n",
    "mega_df['Reporting Period'].fillna(mega_df['ReportingPeriod'],inplace=True)\n",
    "\n",
    "def rep_fix(x):\n",
    "    if \"RP\" in str(x):\n",
    "        return x[-1]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "mega_df['Reporting Period'] = mega_df['Reporting Period'].apply(lambda x: rep_fix(x))\n",
    "mega_df['Reporting Period'] = pd.to_numeric(mega_df['Reporting Period'])\n",
    "\n",
    "mega_df['Date'] = pd.to_datetime(mega_df['Date'])\n",
    "\n",
    "harvest_df = mega_df[['Date',\n",
    "                'Ticket',\n",
    "                'Mill',\n",
    "                'Product',\n",
    "                'Tons (U.S)',\n",
    "                'Cull Tons',\n",
    "                'Accepted Tons',\n",
    "                'Source',\n",
    "                'Notes',\n",
    "                'Reporting Period',\n",
    "                'TicketID',\n",
    "                'Week']].copy()\n",
    "\n",
    "harvest_df = harvest_df.drop_duplicates()\n",
    "harvest_df.loc[harvest_df['TicketID'] == 'WAR18051XXXX', 'TicketID'] = 'WAR180518XXXX' # Found some mislabeled tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_list = harvest_df['Mill'].unique().tolist()\n",
    "mill_dict = {}\n",
    "mill_dict['mill'] = mill_list\n",
    "mill_df = pd.DataFrame(mill_dict)\n",
    "mill_df['mill_uid'] = mill_df.index\n",
    "mill_mapper = dict(zip(mill_df['mill'], mill_df['mill_uid']))\n",
    "harvest_df['mill'] = harvest_df['Mill'].map(mill_mapper)\n",
    "harvest_df.drop(columns=['Mill'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_df['Week'] = harvest_df['Week'].astype(str)\n",
    "harvest_df.loc[harvest_df['Week'] == 'Week3b', 'Week'] = \"3\"\n",
    "def weeker(week):\n",
    "    if \"Week\" in week:\n",
    "        x = week.split(\"k\")[1]\n",
    "        x = int(x)\n",
    "        return x\n",
    "    elif \"nan\" in week:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(week)\n",
    "    \n",
    "harvest_df['Week_trial'] = harvest_df['Week'].apply(lambda x: weeker(x))\n",
    "\n",
    "week_list = harvest_df['Week_trial'].unique().tolist()\n",
    "week_dict = {}\n",
    "week_dict['week'] = week_list\n",
    "week_df = pd.DataFrame(week_dict)\n",
    "week_df['week_uid'] = week_df.index\n",
    "week_mapper = dict(zip(week_df['week'], week_df['week_uid']))\n",
    "harvest_df['week'] = harvest_df['Week_trial'].map(week_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = harvest_df['Product'].unique().tolist()\n",
    "product_dict = {}\n",
    "product_dict['product'] = product_list\n",
    "product_df = pd.DataFrame(product_dict)\n",
    "product_df['product_uid'] = product_df.index\n",
    "product_mapper = dict(zip(product_df['product'], product_df['product_uid']))\n",
    "harvest_df['product'] = harvest_df['Product'].map(product_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_id_mapper = dict(zip(ticketid_df['ticketid'], ticketid_df['ticketid_oid']))\n",
    "harvest_df['ticketid'] = harvest_df['TicketID'].map(ticket_id_mapper)\n",
    "harvest_df['ticketid'] = harvest_df['ticketid'].astype(int)\n",
    "harvest_df.drop(columns=['Product', 'TicketID', 'Week'],inplace=True)\n",
    "harvest_df.rename(columns={'Tons (U.S)': 'total_tons'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to make the Expected Tons / RP table\n",
    "\n",
    "\n",
    "My plan:\n",
    "- Get the previous RP expected simply from MSS \n",
    "- Current RP pull from files I made for Rob\n",
    "\n",
    "Shoot. An issue that I just realized is that the expected needs to be at the unique id. And I aligned the current rps on the ticket id to make it easier for Rob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following should get me up to RP8 for WAR\n",
    "war_expected_inprog = data_df[(~data_df['Reporting Period'].isna()) & (data_df['UniqueID'].str.contains(\"WAR\"))].copy()\n",
    "war_expected_inprog = war_expected_inprog[war_expected_inprog['Reporting Period'] != 9]\n",
    "war_expected_inprog = war_expected_inprog[['TicketID', 'Reporting Period', 'Current RP Expected GT']]\n",
    "war_expected_inprog = war_expected_inprog[war_expected_inprog['Current RP Expected GT'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_expected_inprog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_expected_rp9 = pd.read_excel(r\"./WAR_RP9_InProcess/Excel_sheets/rp9_investigate_matchesRob.xlsx\", sheet_name='Sheet2')\n",
    "war_expected_rp9.rename(columns=({'RP9 Expected': 'Current RP Expected GT'}), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_expected_rp9['Reporting Period'] = 9\n",
    "war_expected_rp9, war_expected_rp9['Current RP Expected GT'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgres_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
