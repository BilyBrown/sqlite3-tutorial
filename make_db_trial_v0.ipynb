{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smartsheet_dataframe import get_as_df\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Smartsheet information in variables\n",
    "shitz_token = \"wMvGniITjDFd7ClZPE44wtJGvZkM7Hg7mP7if\" #Bily's personal token to access G-A smartsheet data. Anyone can obtain their own token\n",
    "fhaid = 4523924143794052 # The ID number that references the FHA sheet\n",
    "mssid = 3005704744265604 # The ID number that references the MSS sheet\n",
    "\n",
    "# Creating the FHA and MSS dataframes\n",
    "fha_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=fhaid)\n",
    "\n",
    "mss_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=mssid)\n",
    "\n",
    "data_df = mss_df.loc[(mss_df['UniqueID'].str.contains('WAR')) |\n",
    "                     (mss_df['UniqueID'].str.contains('WRR')) |\n",
    "                     (mss_df['UniqueID'].str.contains('BJT'))].copy()\n",
    "\n",
    "data_df['Tract'] = data_df['Tract'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "data_df['CMP'] = data_df['CMP'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "data_df['Stand'] = data_df['Stand'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "data_df['TCS'] = data_df['Tract'] + \"_\" + data_df['CMP'] + \"_\" + data_df['Stand']\n",
    "\n",
    "data_df['Reporting Period'] = data_df['Reporting Period'].fillna(0)\n",
    "data_df.loc[data_df['Reporting Period'] == \"\", 'Reporting Period'] = 0\n",
    "s = data_df.loc[pd.to_numeric(data_df['Reporting Period'], errors='coerce').isnull() & data_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'].apply(lambda x: x.strip()[-1])\n",
    "data_df['trial'] = data_df.index.map(s)\n",
    "data_df.loc[pd.to_numeric(data_df['Reporting Period'], errors='coerce').isnull() & data_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'] = data_df.loc[pd.to_numeric(data_df['Reporting Period'], errors='coerce').isnull() & data_df['Reporting Period'].str.contains(\"RP\"), 'trial']\n",
    "data_df['Reporting Period'] = pd.to_numeric(data_df['Reporting Period'], downcast='integer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaet Client Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the Client Provided data and make sure the fields are similar\n",
    "client_df = data_df[['UniqueID',\n",
    "                    'Tract',\n",
    "                    'CMP',\n",
    "                    'Stand',\n",
    "                    'Acres',\n",
    "                    'Age',\n",
    "                    'County',\n",
    "                    'State',\n",
    "                    'Species',\n",
    "                    'Origin',\n",
    "                    'Scheduled',\n",
    "                    'PreBA',\n",
    "                    'PreTPA',\n",
    "                    'PreGTA',\n",
    "                    'PlnBA',\n",
    "                    'PlnTPA',\n",
    "                    'PlnGTA',\n",
    "                    'Client GT',\n",
    "                    'Future GT',\n",
    "                    'Reporting Period']].copy()\n",
    "\n",
    "client_df['Age'] = pd.to_numeric(client_df['Age'], errors='coerce', downcast='integer').fillna(0).astype(int)\n",
    "client_df.loc[pd.to_numeric(client_df['Future GT'], errors='coerce').isnull(), 'Future GT'] = 0\n",
    "client_df['Tract'] = client_df['Tract'].astype(str)\n",
    "client_df['Stand'] = client_df['Stand'].astype(str)\n",
    "client_df['Tract'] = client_df['Tract'].apply(lambda x: x.split('.')[0])\n",
    "client_df['Client GT'] = pd.to_numeric(client_df['Client GT'], errors=\"coerce\")\n",
    "client_df['Client GT'] = client_df['Client GT'].fillna(0)\n",
    "\n",
    "cols = ['PreBA', 'PreTPA', 'PreGTA', 'PlnBA', 'PlnTPA', 'PlnGTA']\n",
    "\n",
    "for x in cols:\n",
    "    client_df[x] = pd.to_numeric(client_df[x], errors='coerce', downcast='integer').fillna(-99).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Normalize some of the fields in the client intake table\n",
    "\n",
    "## Normalize Species\n",
    "species = [\"Slash Pine\", \"Loblolly Pine\", 'Mixed Natural Pine', 'Hardwood Mix', 'Pine Straw', \"Other\"]\n",
    "species_dict = {'Species': species}\n",
    "species_df = pd.DataFrame(species_dict)\n",
    "species_df['Species_UID'] = species_df.index\n",
    "species_df['Species'] = species_df['Species'].str.lower()\n",
    "\n",
    "def species_map(x):\n",
    "    x = x.lower()\n",
    "    if \"slash\" in x:\n",
    "        return 0\n",
    "    elif \"loblol\" in x:\n",
    "        return 1\n",
    "    elif \"mixed natural\" in x:\n",
    "        return 2\n",
    "    elif \"h\" in x:\n",
    "        return 3\n",
    "    elif \"straw\" in x:\n",
    "        return 4\n",
    "    elif \"pine\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "client_df['Species'] = client_df['Species'].apply(lambda x: species_map(x))\n",
    "\n",
    "\n",
    "## Normalize origin\n",
    "origins = ['Planted', 'Natural', 'Natural Mix', 'Other']\n",
    "origin_dict = {'Origin': origins}\n",
    "origin_df = pd.DataFrame(origin_dict)\n",
    "origin_df['OriginUID'] = origin_df.index\n",
    "origin_df['Origin'] = origin_df['Origin'].str.lower()\n",
    "\n",
    "def origin_map(x):\n",
    "    x = x.lower()\n",
    "    if \"plant\" in x:\n",
    "        return 0\n",
    "    elif \"natural mix\" in x:\n",
    "        return 2\n",
    "    elif \"nat\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 3\n",
    "client_df['Origin'] = client_df['Origin'].apply(lambda x: origin_map(x))\n",
    "\n",
    "\n",
    "## Normalize tract\n",
    "client_df['tract_l'] = client_df['Tract'].apply(lambda x: x.lower())\n",
    "tracts = client_df['tract_l'].unique().tolist()\n",
    "tract_dict = {'tract': tracts}\n",
    "tract_df = pd.DataFrame(tract_dict)\n",
    "tract_df['tract_uid'] = tract_df.index\n",
    "new_tract_dict = dict(zip(tract_df['tract'], tract_df['tract_uid']))\n",
    "client_df['Tract'] = client_df['tract_l'].map(new_tract_dict)\n",
    "\n",
    "## Normalize county\n",
    "client_df['County'] = client_df['County'].astype(str)\n",
    "client_df['county_l'] = client_df['County'].apply(lambda x: x.lower())\n",
    "countys = client_df['county_l'].unique().tolist()\n",
    "county_dict = {'county': countys}\n",
    "county_df = pd.DataFrame(county_dict)\n",
    "county_df['county_uid'] = county_df.index\n",
    "new_county_dict = dict(zip(county_df['county'], county_df['county_uid']))\n",
    "client_df['County'] = client_df['county_l'].map(new_county_dict)\n",
    "\n",
    "## Normalize state\n",
    "client_df['State'] = client_df['State'].astype(str)\n",
    "client_df['state_l'] = client_df['State'].apply(lambda x: x.lower())\n",
    "states = client_df['state_l'].unique().tolist()\n",
    "state_dict = {'state': states}\n",
    "state_df = pd.DataFrame(state_dict)\n",
    "state_df['state_uid'] = state_df.index\n",
    "new_state_dict = dict(zip(state_df['state'], state_df['state_uid']))\n",
    "client_df['State'] = client_df['state_l'].map(new_state_dict)\n",
    "\n",
    "# Drop the lowercase columns\n",
    "client_df.drop(columns=['tract_l', 'county_l', 'state_l'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GA Intake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_intake_df = data_df[['IntakeID',\n",
    "                       'FolderID',\n",
    "                       'UniqueID',\n",
    "                       'GA_ACRES',\n",
    "                       'GA GT', # this is going to be GA total isn't it? Or does it stay as GA estimate of current GT (total minus what client says is future)\n",
    "                       'Total GT', # This will be the conservative total between client and GA numbers\n",
    "                       'Notes'\n",
    "                       ]].copy()\n",
    "\n",
    "# Lowercase the ga_intake_df column names\n",
    "ga_intake_df.columns = [x.lower().replace(' ', '_') for x in ga_intake_df.columns]\n",
    "ga_intake_df.rename(columns={'uniqueid': 'ga_uid'},inplace=True)\n",
    "ga_intake_df = ga_intake_df[['ga_uid', 'intakeid', 'folderid', 'ga_acres', 'ga_gt', 'total_gt', 'notes']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Status Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = data_df[['UniqueID',\n",
    "                    'Guidance',\n",
    "                    'Expiration',\n",
    "                    'GuidanceProvider',\n",
    "                    'Status',\n",
    "                    'Completed Date'\n",
    "                    ]].copy()\n",
    "status_df.rename(columns={'UniqueID': 'status_uid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TicketID -> UniqueID lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_lookup_df = data_df[['UniqueID', 'TicketID']].copy() # the uniqueid will the the unique id for this table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketid_list = data_df['TicketID'].unique().tolist()\n",
    "ticketid_dict = {'ticketid': ticketid_list}\n",
    "ticketid_df = pd.DataFrame(ticketid_dict)\n",
    "ticketid_df['ticketid_uid'] = ticketid_df.index\n",
    "new_ticketid_dict = dict(zip(ticketid_df['ticketid'], ticketid_df['ticketid_uid']))\n",
    "ticketid_lookup_df['ticketid'] = ticketid_lookup_df['TicketID'].map(new_ticketid_dict)\n",
    "\n",
    "ticketid_lookup_df.rename(columns={'ticketid_uid': 'ticket_id',\n",
    "                                    'UniqueID': 'ticket_lookup_uid'}, inplace=True)\n",
    "ticketid_lookup_df.drop('TicketID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_lookup_uid</th>\n",
       "      <th>ticketid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WAR241025TT01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WAR241025HD17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WAR241025HD16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WAR241025HD15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WAR241025HD14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>BJT171024TT02</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>BJT171024TT01</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>BJT171024TH07</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>BJT010101TH02</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>BJT010101TH05</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticket_lookup_uid  ticketid\n",
       "10      WAR241025TT01         0\n",
       "11      WAR241025HD17         1\n",
       "12      WAR241025HD16         2\n",
       "13      WAR241025HD15         3\n",
       "14      WAR241025HD14         3\n",
       "..                ...       ...\n",
       "743     BJT171024TT02       454\n",
       "744     BJT171024TT01       455\n",
       "745     BJT171024TH07       456\n",
       "746     BJT010101TH02       457\n",
       "747     BJT010101TH05       458\n",
       "\n",
       "[615 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticketid_lookup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the TCS lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_df = data_df.groupby('TicketID').agg({'Tract': 'first',\n",
    "                                               'CMP': 'first',\n",
    "                                               'Stand': ', '.join}).reset_index()\n",
    "\n",
    "def split_stands(x):\n",
    "    bugger_splot = list(set(x.split(', ')))\n",
    "    bugger_splot.sort()\n",
    "    return \", \".join(bugger_splot)\n",
    "    \n",
    "tcs_df['stands'] = tcs_df['Stand'].apply(split_stands)\n",
    "tcs_df['comp'] = tcs_df['CMP'].apply(lambda x: x if x!=\"NP\" else \"\")\n",
    "tcs_df['TCS'] = tcs_df['Tract'] + \"_\" + tcs_df['comp'] + \"_\" + tcs_df['stands']\n",
    "tcs_df['TCS'] = tcs_df['TCS'].apply(lambda x: x.replace(\"__\", \"_\"))\n",
    "tcs_df['tcs_uid'] = tcs_df.index\n",
    "tcs_df = tcs_df[['tcs_uid', 'TicketID', 'TCS']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TicketID table should have a link to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_mapper = dict(zip(tcs_df['TicketID'], tcs_df['tcs_uid']))\n",
    "ticketid_df['tcs'] = ticketid_df['ticketid'].map(tcs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticketid</th>\n",
       "      <th>ticketid_uid</th>\n",
       "      <th>tcs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ticketid, ticketid_uid, tcs]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticketid_df[ticketid_df['tcs'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place the above tables into the Database\n",
    "- [X] client_df\n",
    "- [X] ga_intake_df\n",
    "- [X] origin_df\n",
    "- [X] county_df\n",
    "- [X] species_df\n",
    "- [X] state_df\n",
    "- [X] status_df\n",
    "- [X] tcs_df\n",
    "- [X] ticketid_df\n",
    "- [X] ticketid_lookup_df\n",
    "- [X] tract_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase all of the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client table\n",
    "## lowercase client_df field names\n",
    "client_df.columns = [x.lower() for x in client_df.columns]\n",
    "client_df.columns = [x.replace(' ', '_') for x in client_df.columns]\n",
    "client_df['client_uid'] = client_df['uniqueid']\n",
    "client_df.drop(columns=['uniqueid'],inplace=True)\n",
    "client_df = client_df[['client_uid', 'tract', 'cmp', 'stand', 'acres', 'age', 'county', 'state', 'species', 'origin', 'scheduled', 'preba', 'pretpa', 'pregta',\n",
    "                       'plnba', 'plntpa', 'plngta', 'client_gt', 'future_gt', 'reporting_period']]\n",
    "\n",
    "# ga_intake\n",
    "ga_intake_df.columns = [x.lower().replace(' ', '_') for x in ga_intake_df.columns]\n",
    "ga_intake_df = ga_intake_df[['ga_uid', 'intakeid', 'folderid', 'ga_acres', 'ga_gt', 'total_gt']]\n",
    "\n",
    "# origin_df\n",
    "origin_df.rename(columns={'OriginUID': 'origin_uid',\n",
    "                          'Origin': 'origin'}, inplace=True)\n",
    "\n",
    "# county_df\n",
    "county_df.columns = [x.lower() for x in county_df.columns]\n",
    "\n",
    "# species_df\n",
    "species_df.columns = [x.lower() for x in species_df.columns]\n",
    "\n",
    "# state_df\n",
    "state_df.columns = [x.lower() for x in state_df.columns]\n",
    "\n",
    "# status_df\n",
    "status_df.columns = [x.lower() for x in status_df.columns]\n",
    "\n",
    "# tcs_df\n",
    "tcs_df.columns = [x.lower() for x in tcs_df.columns]\n",
    "\n",
    "# ticketid_df\n",
    "ticketid_df.columns = [x.lower() for x in ticketid_df.columns]\n",
    "\n",
    "# ticketid_lookup_df\n",
    "ticketid_lookup_df.columns = [x.lower() for x in ticketid_lookup_df.columns]\n",
    "\n",
    "# tract_df\n",
    "tract_df.columns = [x.lower() for x in tract_df.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the db\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(r'C:\\Users\\bbrown\\Documents\\python_scripts\\db-build-trial\\db_trial_v1\\ga_harvest.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Update the DB\n",
    "\n",
    "client_df.to_sql('client_intake', conn, if_exists='replace',index=False, dtype={'client_uid': 'TEXT PRIMARY KEY'})\n",
    "ga_intake_df.to_sql('ga_intake', conn, if_exists='replace', index=False, dtype={'ga_uid': 'TEXT PRIMARY KEY'})\n",
    "origin_df.to_sql('origin', conn, if_exists='replace', index=False, dtype={'origin_uid': 'INTEGER PRIMARY KEY'})\n",
    "county_df.to_sql('county', conn, if_exists='replace', index=False, dtype={'county_uid': 'INTEGER PRIMARY KEY'})\n",
    "species_df.to_sql('species', conn, if_exists='replace', index=False, dtype={'species_uid': 'INTEGER PRIMARY KEY'})\n",
    "state_df.to_sql('state', conn, if_exists='replace', index=False, dtype={'state_uid': 'INTEGER PRIMARY KEY'})\n",
    "status_df.to_sql('status', conn, if_exists='replace', index=False, dtype={'status_uid': 'TEXT PRIMARY KEY'})\n",
    "tcs_df.to_sql('tcs', conn, if_exists='replace', index=False, dtype={'tcs_uid': 'INTEGER PRIMARY KEY'})\n",
    "ticketid_df.to_sql('ticketid', conn, if_exists='replace', index=False, dtype={'ticketid_uid': 'INTEGER PRIMARY KEY'})\n",
    "ticketid_lookup_df.to_sql('ticketid_lookup_activityid', conn, if_exists='replace', index=False, dtype={'ticketid_lookup_uid': 'TEXT PRIMARY KEY'})\n",
    "tract_df.to_sql('tract', conn, if_exists='replace', index=False, dtype={'tract_uid':'INTEGER PRIMARY KEY'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mill Ticket sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "projs = ['WAR', 'WRR', 'BJT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\ARCHIVE\\WAR_RP6\n",
      "WAR_RP6_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WAR_RP7\n",
      "WAR_RP7_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WAR_RP8\n",
      "WAR_RP8_Compiled.xlsm\n",
      "P:\\Mill Tickets\\Retroactive_Ticket_imageing\\WAR\\WAR_RP2\n",
      "WAR_RP2_Compiled.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbrown\\Anaconda3\\envs\\postgres_python\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\Retroactive_Ticket_imageing\\WAR\\WAR_RP3\n",
      "WAR_RP3_Compiled.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbrown\\Anaconda3\\envs\\postgres_python\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\Retroactive_Ticket_imageing\\WAR\\WAR_RP4\n",
      "WAR_RP4_Compiled.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbrown\\Anaconda3\\envs\\postgres_python\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\Retroactive_Ticket_imageing\\WAR\\WAR_RP5\n",
      "WAR_RP5_Compiled.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbrown\\Anaconda3\\envs\\postgres_python\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\WAR_RP9\n",
      "WAR_RP9_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WRR_RP5\n",
      "WRR_RP5_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WRR_RP6\n",
      "WRR_RP6_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WRR_RP7\n",
      "WRR_RP7_Compiled.xlsm\n",
      "P:\\Mill Tickets\\WRR_RP8\n",
      "WRR_RP8_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\BJT_RP5\n",
      "BJT_RP5_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\BJT_RP6\n",
      "BJT_RP6_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\BJT_RP7\n",
      "BJT_RP7_Compiled.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbrown\\AppData\\Local\\Temp\\ipykernel_4352\\2111848845.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  big_df = pd.concat([big_df, df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\BJT_RP8\n",
      "BJT_RP8_Compiled.xlsm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    big_df.loc[big_df[\\'Tons (U.S)\\'].isnull(), \\'Tons (U.S)\\'] = big_df.loc[big_df[\\'Tons (U.S)\\'].isnull(), \\'TonsUS\\']\\n    big_df[\\'Cull Tons\\'].fillna(big_df[\\'CullTons\\'], inplace=True)\\n    big_df[\\'Accepted Tons\\'].fillna(big_df[\\'AcceptedTons\\'], inplace=True)\\n    big_df[\\'Reporting Period\\'].fillna(big_df[\\'ReportingPeriod\\'],inplace=True)\\n\\n    def rep_fix(x):\\n        if \"RP\" in str(x):\\n            return x[-1]\\n        else:\\n            return x\\n        \\n    big_df[\\'Reporting Period\\'] = big_df[\\'Reporting Period\\'].apply(lambda x: rep_fix(x))\\n    big_df[\\'Reporting Period\\'] = pd.to_numeric(big_df[\\'Reporting Period\\'])\\n\\n    big_df[\\'Date\\'] = pd.to_datetime(big_df[\\'Date\\'])\\n\\n    harvest_df = big_df[[\\'Date\\',\\n                    \\'Ticket\\',\\n                    \\'Mill\\',\\n                    \\'Product\\',\\n                    \\'Tons (U.S)\\',\\n                    \\'Cull Tons\\',\\n                    \\'Accepted Tons\\',\\n                    \\'Source\\',\\n                    \\'Notes\\',\\n                    \\'Reporting Period\\',\\n                    \\'TicketID\\',\\n                    \\'Week\\']].copy()\\n\\n    harvest_df = harvest_df.drop_duplicates()\\n    '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mill_loc = r\"P:\\Mill Tickets\"\n",
    "mega_df = pd.DataFrame()\n",
    "\n",
    "for proj in projs:\n",
    "    file_pattern = f\"{proj}_RP*_Compiled.xlsm\"\n",
    "    big_df = pd.DataFrame()\n",
    "\n",
    "    mill_name = []\n",
    "    for root, dirs, files in os.walk(mill_loc):\n",
    "        for file in files:\n",
    "            if f\"{proj}_RP\" in file and file.endswith(\"Compiled.xlsm\"):\n",
    "                if \"$\" in file:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(root)\n",
    "                    print(file)\n",
    "                    df = pd.read_excel(os.path.join(root, file), sheet_name='RP_Project')\n",
    "                    big_df = pd.concat([big_df, df])\n",
    "    mega_df = pd.concat([mega_df, big_df])\n",
    "                    \n",
    "'''\n",
    "    big_df.loc[big_df['Tons (U.S)'].isnull(), 'Tons (U.S)'] = big_df.loc[big_df['Tons (U.S)'].isnull(), 'TonsUS']\n",
    "    big_df['Cull Tons'].fillna(big_df['CullTons'], inplace=True)\n",
    "    big_df['Accepted Tons'].fillna(big_df['AcceptedTons'], inplace=True)\n",
    "    big_df['Reporting Period'].fillna(big_df['ReportingPeriod'],inplace=True)\n",
    "\n",
    "    def rep_fix(x):\n",
    "        if \"RP\" in str(x):\n",
    "            return x[-1]\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    big_df['Reporting Period'] = big_df['Reporting Period'].apply(lambda x: rep_fix(x))\n",
    "    big_df['Reporting Period'] = pd.to_numeric(big_df['Reporting Period'])\n",
    "\n",
    "    big_df['Date'] = pd.to_datetime(big_df['Date'])\n",
    "\n",
    "    harvest_df = big_df[['Date',\n",
    "                    'Ticket',\n",
    "                    'Mill',\n",
    "                    'Product',\n",
    "                    'Tons (U.S)',\n",
    "                    'Cull Tons',\n",
    "                    'Accepted Tons',\n",
    "                    'Source',\n",
    "                    'Notes',\n",
    "                    'Reporting Period',\n",
    "                    'TicketID',\n",
    "                    'Week']].copy()\n",
    "\n",
    "    harvest_df = harvest_df.drop_duplicates()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_df.loc[mega_df['Tons (U.S)'].isnull(), 'Tons (U.S)'] = mega_df.loc[mega_df['Tons (U.S)'].isnull(), 'TonsUS']\n",
    "mega_df['Cull Tons'].fillna(mega_df['CullTons'], inplace=True)\n",
    "mega_df['Accepted Tons'].fillna(mega_df['AcceptedTons'], inplace=True)\n",
    "mega_df['Reporting Period'].fillna(mega_df['ReportingPeriod'],inplace=True)\n",
    "\n",
    "def rep_fix(x):\n",
    "    if \"RP\" in str(x):\n",
    "        return x[-1]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "mega_df['Reporting Period'] = mega_df['Reporting Period'].apply(lambda x: rep_fix(x))\n",
    "mega_df['Reporting Period'] = pd.to_numeric(mega_df['Reporting Period'])\n",
    "\n",
    "mega_df['Date'] = pd.to_datetime(mega_df['Date'])\n",
    "\n",
    "harvest_df = mega_df[['Date',\n",
    "                'Ticket',\n",
    "                'Mill',\n",
    "                'Product',\n",
    "                'Tons (U.S)',\n",
    "                'Cull Tons',\n",
    "                'Accepted Tons',\n",
    "                'Source',\n",
    "                'Notes',\n",
    "                'Reporting Period',\n",
    "                'TicketID',\n",
    "                'Week']].copy()\n",
    "\n",
    "harvest_df = harvest_df.drop_duplicates()\n",
    "harvest_df.loc[harvest_df['TicketID'] == 'WAR18051XXXX', 'TicketID'] = 'WAR180518XXXX' # Found some mislabeled tickets\n",
    "harvest_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_list = harvest_df['Mill'].unique().tolist()\n",
    "mill_dict = {}\n",
    "mill_dict['mill'] = mill_list\n",
    "mill_df = pd.DataFrame(mill_dict)\n",
    "mill_df['mill_uid'] = mill_df.index\n",
    "mill_mapper = dict(zip(mill_df['mill'], mill_df['mill_uid']))\n",
    "harvest_df['mill'] = harvest_df['Mill'].map(mill_mapper)\n",
    "harvest_df.drop(columns=['Mill'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_df['Week'] = harvest_df['Week'].astype(str)\n",
    "harvest_df.loc[harvest_df['Week'] == 'Week3b', 'Week'] = \"3\"\n",
    "def weeker(week):\n",
    "    if \"Week\" in week:\n",
    "        x = week.split(\"k\")[1]\n",
    "        x = int(x)\n",
    "        return x\n",
    "    elif \"nan\" in week:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(week)\n",
    "    \n",
    "harvest_df['Week_trial'] = harvest_df['Week'].apply(lambda x: weeker(x))\n",
    "\n",
    "# I am not sure if I want to normalize the week or just leave it as the integer it is...\n",
    "'''week_list = harvest_df['Week_trial'].unique().tolist()\n",
    "week_dict = {}\n",
    "week_dict['week'] = week_list\n",
    "week_df = pd.DataFrame(week_dict)\n",
    "week_df['week_uid'] = week_df.index\n",
    "week_mapper = dict(zip(week_df['week'], week_df['week_uid']))\n",
    "harvest_df['week'] = harvest_df['Week_trial'].map(week_mapper)'''\n",
    "\n",
    "harvest_df['week'] = harvest_df['Week_trial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = harvest_df['Product'].unique().tolist()\n",
    "product_dict = {}\n",
    "product_dict['product'] = product_list\n",
    "product_df = pd.DataFrame(product_dict)\n",
    "product_df['product_uid'] = product_df.index\n",
    "product_mapper = dict(zip(product_df['product'], product_df['product_uid']))\n",
    "harvest_df['product'] = harvest_df['Product'].map(product_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_id_mapper = dict(zip(ticketid_df['ticketid'], ticketid_df['ticketid_uid']))\n",
    "harvest_df['ticketid'] = harvest_df['TicketID'].map(ticket_id_mapper)\n",
    "harvest_df['ticketid'] = harvest_df['ticketid'].astype(int)\n",
    "harvest_df.drop(columns=['Product', 'TicketID', 'Week', 'Week_trial', 'index'],inplace=True)\n",
    "harvest_df.rename(columns={'Tons (U.S)': 'total_tons'})\n",
    "harvest_df['harvest_uid'] = harvest_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Tons (U.S)</th>\n",
       "      <th>Cull Tons</th>\n",
       "      <th>Accepted Tons</th>\n",
       "      <th>Source</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Reporting Period</th>\n",
       "      <th>mill</th>\n",
       "      <th>week</th>\n",
       "      <th>product</th>\n",
       "      <th>ticketid</th>\n",
       "      <th>harvest_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>160048</td>\n",
       "      <td>26.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.31</td>\n",
       "      <td>M-82_Week 1.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>160157</td>\n",
       "      <td>25.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.79</td>\n",
       "      <td>M-82_Week 1.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>160120</td>\n",
       "      <td>25.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.32</td>\n",
       "      <td>M-82_Week 1.pdf</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>160155</td>\n",
       "      <td>25.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.31</td>\n",
       "      <td>M-82_Week 1.pdf</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>160082</td>\n",
       "      <td>27.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.17</td>\n",
       "      <td>M-82_Week 1.pdf</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48801</th>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>21640715</td>\n",
       "      <td>30.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.05</td>\n",
       "      <td>2024BJT39wk2.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>48801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48802</th>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>21644553</td>\n",
       "      <td>32.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.88</td>\n",
       "      <td>2024BJT39wk2.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>48802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48803</th>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>21644635</td>\n",
       "      <td>26.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.47</td>\n",
       "      <td>2024BJT39wk2.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>48803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48804</th>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>21643437</td>\n",
       "      <td>32.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.48</td>\n",
       "      <td>2024BJT39wk2.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>48804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48805</th>\n",
       "      <td>2024-07-25</td>\n",
       "      <td>21645207</td>\n",
       "      <td>28.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.84</td>\n",
       "      <td>2024BJT39wk2.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>48805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48806 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Ticket  Tons (U.S)  Cull Tons  Accepted Tons  \\\n",
       "0     2021-08-02    160048       26.31        0.0          26.31   \n",
       "1     2021-08-03    160157       25.79        0.0          25.79   \n",
       "2     2021-08-03    160120       25.32        0.0          25.32   \n",
       "3     2021-08-03    160155       25.31        0.0          25.31   \n",
       "4     2021-08-03    160082       27.17        0.0          27.17   \n",
       "...          ...       ...         ...        ...            ...   \n",
       "48801 2024-07-15  21640715       30.05        0.0          30.05   \n",
       "48802 2024-07-24  21644553       32.88        0.0          32.88   \n",
       "48803 2024-07-24  21644635       26.47        0.0          26.47   \n",
       "48804 2024-07-22  21643437       32.48        0.0          32.48   \n",
       "48805 2024-07-25  21645207       28.84        0.0          28.84   \n",
       "\n",
       "                 Source Notes  Reporting Period  mill  week  product  \\\n",
       "0       M-82_Week 1.pdf   NaN                 6     0     1        0   \n",
       "1       M-82_Week 1.pdf   NaN                 6     0     1        0   \n",
       "2       M-82_Week 1.pdf                       6     0     1        0   \n",
       "3       M-82_Week 1.pdf                       6     0     1        0   \n",
       "4       M-82_Week 1.pdf                       6     0     1        0   \n",
       "...                 ...   ...               ...   ...   ...      ...   \n",
       "48801  2024BJT39wk2.pdf   NaN                 8    40     4        0   \n",
       "48802  2024BJT39wk2.pdf   NaN                 8    40     4        0   \n",
       "48803  2024BJT39wk2.pdf   NaN                 8    40     4        0   \n",
       "48804  2024BJT39wk2.pdf   NaN                 8    40     4        0   \n",
       "48805  2024BJT39wk2.pdf   NaN                 8    40     4        0   \n",
       "\n",
       "       ticketid  harvest_uid  \n",
       "0           375            0  \n",
       "1           375            1  \n",
       "2           375            2  \n",
       "3           375            3  \n",
       "4           375            4  \n",
       "...         ...          ...  \n",
       "48801       432        48801  \n",
       "48802       432        48802  \n",
       "48803       432        48803  \n",
       "48804       432        48804  \n",
       "48805       432        48805  \n",
       "\n",
       "[48806 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send harvest data to db\n",
    "\n",
    "harvest_df.to_sql('harvest_vol', conn, if_exists='replace', index=False, dtype={'harvest_uid':'INTEGER PRIMARY KEY'})\n",
    "product_df.to_sql('product', conn, if_exists='replace', index=False, dtype={'product_uid': 'INTEGER PRIMARY KEY'})\n",
    "mill_df.to_sql('mill', conn, if_exists='replace', index=False, dtype={'mill_uid': 'INTEGER PRIMARY KEY'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to make the Expected Tons / RP table\n",
    "\n",
    "\n",
    "My plan:\n",
    "- Get the previous RP expected simply from MSS \n",
    "- Current RP pull from files I made for Rob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following should get me up to RP8 for WAR\n",
    "war_expected_inprog = data_df[(~data_df['Reporting Period'].isna()) & (data_df['UniqueID'].str.contains(\"WAR\"))].copy()\n",
    "war_expected_inprog = war_expected_inprog[war_expected_inprog['Reporting Period'] != 9]\n",
    "war_expected_inprog = war_expected_inprog[['TicketID', 'Reporting Period', 'Current RP Expected GT']]\n",
    "war_expected_inprog = war_expected_inprog[war_expected_inprog['Current RP Expected GT'] != \"\"]\n",
    "war_expected_inprog = war_expected_inprog.groupby(['TicketID', 'Reporting Period'])['Current RP Expected GT'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TicketID</th>\n",
       "      <th>Reporting Period</th>\n",
       "      <th>Current RP Expected GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WAR211101CC01</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WAR220728XXXX</td>\n",
       "      <td>8</td>\n",
       "      <td>6177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WAR220729TH01</td>\n",
       "      <td>8</td>\n",
       "      <td>17604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WAR220801CC01</td>\n",
       "      <td>8</td>\n",
       "      <td>3268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WAR220801CC03</td>\n",
       "      <td>8</td>\n",
       "      <td>10324.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TicketID  Reporting Period Current RP Expected GT\n",
       "0  WAR211101CC01                 6                    0.0\n",
       "1  WAR220728XXXX                 8                 6177.0\n",
       "2  WAR220729TH01                 8                17604.0\n",
       "3  WAR220801CC01                 8                 3268.0\n",
       "4  WAR220801CC03                 8                10324.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "war_expected_inprog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_expected_rp9 = pd.read_excel(r\"./WAR_RP9_InProcess/Excel_sheets/rp9_investigate_matchesRob.xlsx\", sheet_name='Sheet2')\n",
    "war_expected_rp9.rename(columns=({'RP9 Expected': 'Current RP Expected GT'}), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         TicketID  Current RP Expected GT  Reporting Period\n",
       " 0   WAR230908XXXX                 9019.00                 9\n",
       " 1   WAR231107XXXX                12289.71                 9\n",
       " 2   WAR240521CCXX                 9814.68                 9\n",
       " 3   WAR240521TN01                 2918.65                 9\n",
       " 4   WAR240521TT01                 6756.53                 9\n",
       " ..            ...                     ...               ...\n",
       " 68  WAR241025HD04                 5186.26                 9\n",
       " 69  WAR241025HD03                 2700.00                 9\n",
       " 70  WAR241025HD02                 8592.47                 9\n",
       " 71  WAR241025HD01                 1200.00                 9\n",
       " 72  WAR230318TXXX                    0.00                 9\n",
       " \n",
       " [73 rows x 3 columns],\n",
       " 424209.6018388594)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "war_expected_rp9['Reporting Period'] = 9\n",
    "war_expected_rp9, war_expected_rp9['Current RP Expected GT'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two together - though it will not account for any carryover pre- RP8 ( i am pretty sure I brute forced the RP8 to be correct for reporting purposes)\n",
    "war_expected = pd.concat([war_expected_inprog, war_expected_rp9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do WRR\n",
    "wrr_expected_inprog = data_df[(~data_df['Reporting Period'].isna()) & (data_df['UniqueID'].str.contains(\"WRR\"))].copy()\n",
    "wrr_expected_inprog = wrr_expected_inprog[wrr_expected_inprog['Reporting Period'] != 8]\n",
    "wrr_expected_inprog = wrr_expected_inprog[['TicketID', 'Reporting Period', 'Current RP Expected GT']]\n",
    "wrr_expected_inprog = wrr_expected_inprog[wrr_expected_inprog['Current RP Expected GT'] != \"\"]\n",
    "wrr_expected_inprog = wrr_expected_inprog.groupby(['TicketID', 'Reporting Period'])['Current RP Expected GT'].sum().reset_index()\n",
    "\n",
    "# Current rp\n",
    "wrr_current_rp = pd.read_excel(r\"C:\\Users\\bbrown\\Documents\\python_scripts\\db-build-trial\\WRR_RP8_InProcess\\WRR_to_db_241202.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrr_expected = pd.concat([wrr_expected_inprog, wrr_current_rp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84259.66040039062"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrr_expected.loc[wrr_expected['Reporting Period'] == 8, 'Current RP Expected GT'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for BJT\n",
    "# previous rp expected\n",
    "bjt_expected_inprog = data_df[(~data_df['Reporting Period'].isna()) & (data_df['UniqueID'].str.contains(\"BJT\"))].copy()\n",
    "bjt_expected_inprog = bjt_expected_inprog[bjt_expected_inprog['Reporting Period'] != 8]\n",
    "bjt_expected_inprog = bjt_expected_inprog[['TicketID', 'Reporting Period', 'Current RP Expected GT']]\n",
    "bjt_expected_inprog = bjt_expected_inprog[bjt_expected_inprog['Current RP Expected GT'] != '']\n",
    "bjt_expected_inprog = bjt_expected_inprog.groupby(['TicketID', 'Reporting Period'])['Current RP Expected GT'].sum().reset_index()\n",
    "\n",
    "# current rp expected\n",
    "bjt_current_rp = pd.read_excel(r\"C:\\Users\\bbrown\\Documents\\python_scripts\\db-build-trial\\BJT_RP8_InProcess\\BJT_RP8_requestvol_241202.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bjt_expected_df = pd.concat([bjt_expected_inprog, bjt_current_rp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all of the expected into a single table\n",
    "\n",
    "total_expected = pd.concat([war_expected, wrr_expected, bjt_expected_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_expected.reset_index(inplace=True)\n",
    "total_expected['gt_expected_uid'] = total_expected.index\n",
    "# Normalize the ticketid\n",
    "total_expected['ticketid'] = total_expected['TicketID'].map(ticket_id_mapper)\n",
    "total_expected.to_excel(\"expected_lookup.xlsx\")\n",
    "total_expected['ticketid'] = total_expected['ticketid'].astype(int)\n",
    "total_expected = total_expected.rename(columns={'Reporting Period': 'reporting_period', 'Current RP Expected GT': 'expected_gt'})\n",
    "total_expected = total_expected[['gt_expected_uid', 'ticketid', 'reporting_period', 'expected_gt']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_expected.to_sql('gt_expected', conn, index=False, if_exists='replace', dtype={'gt_expected_uid': 'INTEGER PRIMARY KEY'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgres_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
