{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smartsheet_dataframe import get_as_df\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('./WAR_RP9_InProcess/trial-db/Harvest.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Smartsheet information in variables\n",
    "shitz_token = \"wMvGniITjDFd7ClZPE44wtJGvZkM7Hg7mP7if\" #Bily's personal token to access G-A smartsheet data. Anyone can obtain their own token\n",
    "fhaid = 4523924143794052 # The ID number that references the FHA sheet\n",
    "mssid = 3005704744265604 # The ID number that references the MSS sheet\n",
    "\n",
    "# Creating the FHA and MSS dataframes\n",
    "fha_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=fhaid)\n",
    "\n",
    "mss_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=mssid)\n",
    "\n",
    "war_df = mss_df.loc[mss_df['UniqueID'].str.contains(\"WAR\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\ARCHIVE\\WAR_RP6\n",
      "WAR_RP6_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WAR_RP7\n",
      "WAR_RP7_Compiled.xlsm\n",
      "P:\\Mill Tickets\\ARCHIVE\\WAR_RP8\n",
      "WAR_RP8_Compiled.xlsm\n",
      "P:\\Mill Tickets\\Retroactive_Ticket_imageing\\WAR\\WAR_RP2\n",
      "WAR_RP2_Compiled.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbrown\\Anaconda3\\envs\\postgres_python\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\Retroactive_Ticket_imageing\\WAR\\WAR_RP3\n",
      "WAR_RP3_Compiled.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbrown\\Anaconda3\\envs\\postgres_python\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\Mill Tickets\\Retroactive_Ticket_imageing\\WAR\\WAR_RP4\n",
      "WAR_RP4_Compiled.xlsm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbrown\\Anaconda3\\envs\\postgres_python\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "mill_loc = r\"P:\\Mill Tickets\"\n",
    "file_pattern = \"WAR_RP*_Compiled.xlsm\"\n",
    "big_df = pd.DataFrame()\n",
    "\n",
    "mill_name = []\n",
    "for root, dirs, files in os.walk(mill_loc):\n",
    "    for file in files:\n",
    "        if \"WAR_RP\" in file and file.endswith(\"Compiled.xlsm\"):\n",
    "            if \"$\" in file:\n",
    "                pass\n",
    "            else:\n",
    "                print(root)\n",
    "                print(file)\n",
    "                df = pd.read_excel(os.path.join(root, file), sheet_name='RP_Project')\n",
    "                big_df = pd.concat([big_df, df])\n",
    "                \n",
    "\n",
    "big_df.loc[big_df['Tons (U.S)'].isnull(), 'Tons (U.S)'] = big_df.loc[big_df['Tons (U.S)'].isnull(), 'TonsUS']\n",
    "big_df['Cull Tons'].fillna(big_df['CullTons'], inplace=True)\n",
    "big_df['Accepted Tons'].fillna(big_df['AcceptedTons'], inplace=True)\n",
    "big_df['Reporting Period'].fillna(big_df['ReportingPeriod'],inplace=True)\n",
    "\n",
    "def rep_fix(x):\n",
    "    if \"RP\" in str(x):\n",
    "        return x[-1]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "big_df['Reporting Period'] = big_df['Reporting Period'].apply(lambda x: rep_fix(x))\n",
    "big_df['Reporting Period'] = pd.to_numeric(big_df['Reporting Period'])\n",
    "\n",
    "big_df['Date'] = pd.to_datetime(big_df['Date'])\n",
    "\n",
    "harvest_df = big_df[['Date',\n",
    "                  'Ticket',\n",
    "                  'Mill',\n",
    "                  'Product',\n",
    "                  'Tons (U.S)',\n",
    "                  'Cull Tons',\n",
    "                  'Accepted Tons',\n",
    "                  'Source',\n",
    "                  'Notes',\n",
    "                  'Reporting Period',\n",
    "                  'TicketID',\n",
    "                  'Week']].copy()\n",
    "\n",
    "harvest_df = harvest_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_df.loc[harvest_df['TicketID'] == 'WAR18051XXXX', 'TicketID'] = 'WAR180518XXXX' # Found some mislabeled tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for the mill ticket normalization\n",
    "# Normalize the Weeks column.\n",
    "# Normalize the source column\n",
    "# Normalize the Mill column\n",
    "# Normalize the Product column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(harvest_df['Mill'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(harvest_df['Mill'].str.lower().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mill_list = harvest_df['Mill'].unique().tolist()\n",
    "mill_dict = {}\n",
    "mill_dict['mill'] = mill_list\n",
    "mill_df = pd.DataFrame(mill_dict)\n",
    "mill_df['mill_uid'] = mill_df.index\n",
    "mill_mapper = dict(zip(mill_df['mill'], mill_df['mill_uid']))\n",
    "harvest_df['mill'] = harvest_df['Mill'].map(mill_mapper)\n",
    "harvest_df.drop(columns=['Mill'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_df['Week'] = harvest_df['Week'].astype(str)\n",
    "harvest_df.loc[harvest_df['Week'] == 'Week3b', 'Week'] = \"3\"\n",
    "def weeker(week):\n",
    "    if \"Week\" in week:\n",
    "        x = week.split(\"k\")[1]\n",
    "        x = int(x)\n",
    "        return x\n",
    "    else:\n",
    "        return int(week)\n",
    "    \n",
    "harvest_df['Week_trial'] = harvest_df['Week'].apply(lambda x: weeker(x))\n",
    "\n",
    "week_list = harvest_df['Week_trial'].unique().tolist()\n",
    "week_dict = {}\n",
    "week_dict['week'] = week_list\n",
    "week_df = pd.DataFrame(week_dict)\n",
    "week_df['week_uid'] = week_df.index\n",
    "week_mapper = dict(zip(week_df['week'], week_df['week_uid']))\n",
    "harvest_df['week'] = harvest_df['Week_trial'].map(week_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = harvest_df['Product'].unique().tolist()\n",
    "product_dict = {}\n",
    "product_dict['product'] = product_list\n",
    "product_df = pd.DataFrame(product_dict)\n",
    "product_df['product_uid'] = product_df.index\n",
    "product_mapper = dict(zip(product_df['product'], product_df['product_uid']))\n",
    "harvest_df['product'] = harvest_df['Product'].map(product_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ticketid from sql and see how that shakes out\n",
    "ticketid_df = pd.read_sql('SELECT * FROM ticketid',conn)\n",
    "ticket_id_mapper = dict(zip(ticketid_df['ticketid'], ticketid_df['id']))\n",
    "harvest_df['ticketid'] = harvest_df['TicketID'].map(ticket_id_mapper)\n",
    "harvest_df['ticketid'] = harvest_df['ticketid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_df.drop(columns=['Product', 'TicketID', 'Week'],inplace=True)\n",
    "harvest_df.rename(columns={'Tons (U.S)': 'total_tons'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_df.columns = [x.lower() for x in harvest_df.columns]\n",
    "harvest_df.columns = [x.replace(' ', '_') for x in harvest_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_df[harvest_df['reporting_period'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgres_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
