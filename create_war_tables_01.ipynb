{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smartsheet_dataframe import get_as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Smartsheet information in variables\n",
    "shitz_token = \"wMvGniITjDFd7ClZPE44wtJGvZkM7Hg7mP7if\" #Bily's personal token to access G-A smartsheet data. Anyone can obtain their own token\n",
    "fhaid = 4523924143794052 # The ID number that references the FHA sheet\n",
    "mssid = 3005704744265604 # The ID number that references the MSS sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FHA and MSS dataframes\n",
    "fha_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=fhaid)\n",
    "\n",
    "mss_df = get_as_df(type_='sheet',\n",
    "                  token=shitz_token,\n",
    "                  id_=mssid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_df = mss_df.loc[mss_df['UniqueID'].str.contains(\"WAR\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df = war_df[['UniqueID',\n",
    "                    'Tract',\n",
    "                    'CMP',\n",
    "                    'Stand',\n",
    "                    'Acres',\n",
    "                    'Age',\n",
    "                    'County',\n",
    "                    'State',\n",
    "                    'Stand Type',\n",
    "                    'Origin',\n",
    "                    'Scheduled',\n",
    "                    'PreBA',\n",
    "                    'PreTPA',\n",
    "                    'PreGTA',\n",
    "                    'PlnBA',\n",
    "                    'PlnTPA',\n",
    "                    'PlnGTA',\n",
    "                    'Client GT',\n",
    "                    'Future GT',\n",
    "                    'Reporting Period']].copy()\n",
    "\n",
    "client_df['Reporting Period'] = client_df['Reporting Period'].fillna(0)\n",
    "client_df.loc[client_df['Reporting Period'] == \"\", 'Reporting Period'] = 0\n",
    "s = client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'].apply(lambda x: x.strip()[-1])\n",
    "client_df['trial'] = client_df.index.map(s)\n",
    "client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'Reporting Period'] = client_df.loc[pd.to_numeric(client_df['Reporting Period'], errors='coerce').isnull() & client_df['Reporting Period'].str.contains(\"RP\"), 'trial']\n",
    "client_df['Reporting Period'] = pd.to_numeric(client_df['Reporting Period'], downcast='integer')\n",
    "client_df['Age'] = pd.to_numeric(client_df['Age'], errors='coerce', downcast='integer').fillna(0).astype(int)\n",
    "client_df.loc[pd.to_numeric(client_df['Future GT'], errors='coerce').isnull(), 'Future GT'] = 0\n",
    "client_df['Tract'] = client_df['Tract'].astype(str)\n",
    "client_df['Stand'] = client_df['Stand'].astype(str)\n",
    "client_df['Tract'] = client_df['Tract'].apply(lambda x: x.split('.')[0])\n",
    "client_df['Client GT'] = pd.to_numeric(client_df['Client GT'], errors=\"coerce\")\n",
    "client_df['Client GT'] = client_df['Client GT'].fillna(0)\n",
    "\n",
    "cols = ['PreBA', 'PreTPA', 'PreGTA', 'PlnBA', 'PlnTPA', 'PlnGTA']\n",
    "\n",
    "for x in cols:\n",
    "    client_df[x] = pd.to_numeric(client_df[x], errors='coerce', downcast='integer').fillna(-99).astype(int)\n",
    "\n",
    "client_df.drop(columns=['trial'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am thinking of creating more tables : county, state, origin, species. That way if we have to update the way somethign is spelled we just do it in one place.\n",
    "\n",
    "Maybe even tract as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = [\"Slash Pine\", \"Loblolly Pine\", 'Mixed Natural Pine', 'Hardwood Mix', 'Pine Straw', \"Other\"]\n",
    "species_dict = {'Species': species}\n",
    "species_df = pd.DataFrame(species_dict)\n",
    "species_df['SpeciesUID'] = species_df.index\n",
    "\n",
    "def species_map(x):\n",
    "    x = x.lower()\n",
    "    if \"slash\" in x:\n",
    "        return 0\n",
    "    elif \"loblol\" in x:\n",
    "        return 1\n",
    "    elif \"mixed natural\" in x:\n",
    "        return 2\n",
    "    elif \"h\" in x:\n",
    "        return 3\n",
    "    elif \"straw\" in x:\n",
    "        return 4\n",
    "    elif \"pine\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "client_df['Stand Type'] = client_df['Stand Type'].apply(lambda x: species_map(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = ['Planted', 'Natural', 'Natural Mix', 'Other']\n",
    "origin_dict = {'Origin': origins}\n",
    "origin_df = pd.DataFrame(origin_dict)\n",
    "origin_df['OriginUID'] = origin_df.index\n",
    "\n",
    "def origin_map(x):\n",
    "    x = x.lower()\n",
    "    if \"plant\" in x:\n",
    "        return 0\n",
    "    elif \"natural mix\" in x:\n",
    "        return 2\n",
    "    elif \"nat\" in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 3\n",
    "client_df['Origin'] = client_df['Origin'].apply(lambda x: origin_map(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_intake_df = war_df[['IntakeID',\n",
    "                       'FolderID',\n",
    "                       'UniqueID',\n",
    "                       'GA_ACRES',\n",
    "                       'GA GT', # this is going to be GA total isn't it? Or does it stay as GA estimate of current GT (total minus what client says is future)\n",
    "                       'Total GT', # Does this need included? At the moment we do funny stuff with total gt and subtracting future gt to get GA current rp (above)\n",
    "                       'Notes'\n",
    "                       ]].copy()\n",
    "\n",
    "# Do I want to include a Ticketid column here or use a crossreference table?\n",
    "# Should reporting period be in here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = war_df[['UniqueID',\n",
    "                    'Guidance',\n",
    "                    'Expiration',\n",
    "                    'GuidanceProvider',\n",
    "                    'Status',\n",
    "                    'Completed Date'\n",
    "                    ]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intake_breaker(lst):\n",
    "    intake_lst = []\n",
    "    activity_lst = []\n",
    "    for ele in lst:\n",
    "        intake_lst.append(ele[:9])\n",
    "        activity_lst.append(ele[9:11])\n",
    "        intake_st = set(intake_lst)\n",
    "        activity_st = set(activity_lst)\n",
    "        \n",
    "    return intake_lst, activity_lst, intake_st, activity_st\n",
    "\n",
    "\n",
    "def ticketid(lst):\n",
    "    \"\"\"This function creates a ticket id based off a list of unique ids\n",
    "\n",
    "    Args:\n",
    "        lst (list): a list of unique ids that have the same Tract_Cmp_Stand from MSS\n",
    "    \"\"\"\n",
    "    if len(lst) == 0:\n",
    "        return \"ERROR, Where is your uniqueid?\"\n",
    "    elif len(lst) < 2:\n",
    "        return lst[0]\n",
    "    elif len(lst) > 1:\n",
    "        # Check to see if there are multiple intakes\n",
    "        temp_lst, activity_lst, temp_st, activity_st = intake_breaker(lst)\n",
    "        temp_lst.sort()\n",
    "        activity_lst.sort()\n",
    "        if len(temp_st) > 1:\n",
    "            return f\"{temp_lst[0]}XXXX\"\n",
    "        # If there are not multiple intakes. Check to see if there are multiple activities in the lst (Intake portion should be same for all)\n",
    "        elif len(activity_st) > 1:\n",
    "           return f\"{temp_lst[0]}XXXX\"\n",
    "       # If there is only a single activity type\n",
    "        elif len(activity_st) == 1:\n",
    "            return f\"{temp_lst[0]}{activity_lst[0]}XX\"\n",
    "        else:\n",
    "            return \"Failed within len(lst) > 1\"\n",
    "        \n",
    "    else:\n",
    "        return \"Failed at length of lst\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_inprog = war_df[war_df['Status'].isin(['OPEN', 'IN PROCESS', 'POST', 'PENDING'])].copy()\n",
    "war_inprog['Tract'] = war_inprog['Tract'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "war_inprog['CMP'] = war_inprog['CMP'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "war_inprog['Stand'] = war_inprog['Stand'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "war_inprog['TCS'] = war_inprog['Tract'] + \"_\" + war_inprog['CMP'] + \"_\" + war_inprog['Stand']\n",
    "\n",
    "# After talking with Rob, we need to make sure that we separate out the GA and the ARB Guidance\n",
    "grouped_activities = war_inprog.groupby(['TCS', 'GuidanceProvider'])['UniqueID'].apply(list).reset_index()\n",
    "\n",
    "trial_df = grouped_activities.copy()\n",
    "trial_df['BBID'] = trial_df['UniqueID'].apply(lambda x: ticketid(x))\n",
    "\n",
    "prev_combined = war_inprog.groupby('TicketID')['TCS'].apply(set).reset_index()\n",
    "\n",
    "tcs = []\n",
    "for val in prev_combined['TCS'].loc[prev_combined['TCS'].str.len()> 1].values:\n",
    "    for item in val:\n",
    "        tcs.append(item)\n",
    "        \n",
    "ticket_id_dict = {}\n",
    "for i, row in prev_combined.loc[prev_combined['TCS'].str.len()> 1].iterrows():\n",
    "    # print(row['TicketID']) # ticket id\n",
    "    value_list = []\n",
    "    for value in row['TCS']:\n",
    "        value_list.append(value)\n",
    "    ticket_id_dict[row['TicketID']] = value_list\n",
    "    \n",
    "for k, v in ticket_id_dict.items():\n",
    "    trial_df.loc[trial_df['TCS'].isin(v), 'otherid'] = k\n",
    "\n",
    "def final_id(row):\n",
    "    if row['otherid'] != \"nan\":\n",
    "        return row['otherid']\n",
    "    else:\n",
    "        return row['BBID']\n",
    "    \n",
    "trial_df['finalid'] = trial_df[['BBID', 'otherid']].apply(final_id, axis=1)\n",
    "\n",
    "def break_list(x):\n",
    "    trial_list = []\n",
    "    for item in x:\n",
    "        for iter in item:\n",
    "            trial_list.append(iter)\n",
    "            \n",
    "    return trial_list\n",
    "\n",
    "new_ticket_df = trial_df.groupby('finalid').agg({'TCS': lambda x: list(x),\n",
    "                                 'UniqueID': lambda x: break_list(x)}).reset_index()\n",
    "\n",
    "client_for_ticket = client_df.copy()\n",
    "for id in new_ticket_df['finalid'].unique().tolist():\n",
    "    client_for_ticket.loc[client_for_ticket['UniqueID'].isin(new_ticket_df.loc[new_ticket_df['finalid'] == id, 'UniqueID'].values[0]), 'TicketID'] = id\n",
    "\n",
    "new_tcs_intake = client_for_ticket.groupby(['TicketID',\n",
    "                                            'UniqueID'])['Client GT'].sum().reset_index()\n",
    "\n",
    "old_ticket_ids = dict(zip(war_df['UniqueID'], war_df['TicketID']))\n",
    "new_tcs_intake['old_ticketids'] = new_tcs_intake['UniqueID'].map(old_ticket_ids)\n",
    "new_tcs_intake.loc[~new_tcs_intake['TicketID'].str.contains(\"WAR\"), 'TicketID'] = new_tcs_intake['old_ticketids']\n",
    "\n",
    "ticketid_table = new_tcs_intake[['TicketID', 'UniqueID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the ticket table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "mill_loc = r\"P:\\Mill Tickets\"\n",
    "file_pattern = \"WAR_RP*_Compiled.xlsm\"\n",
    "big_df = pd.DataFrame()\n",
    "\n",
    "mill_name = []\n",
    "for root, dirs, files in os.walk(mill_loc):\n",
    "    for file in files:\n",
    "        if \"WAR_RP\" in file and file.endswith(\"Compiled.xlsm\"):\n",
    "            if \"$\" in file:\n",
    "                pass\n",
    "            else:\n",
    "                print(root)\n",
    "                print(file)\n",
    "                df = pd.read_excel(os.path.join(root, file), sheet_name='RP_Project')\n",
    "                big_df = pd.concat([big_df, df])\n",
    "                \n",
    "\n",
    "big_df.loc[big_df['Tons (U.S)'].isnull(), 'Tons (U.S)'] = big_df.loc[big_df['Tons (U.S)'].isnull(), 'TonsUS']\n",
    "big_df['Cull Tons'].fillna(big_df['CullTons'], inplace=True)\n",
    "big_df['Accepted Tons'].fillna(big_df['AcceptedTons'], inplace=True)\n",
    "big_df['Reporting Period'].fillna(big_df['ReportingPeriod'],inplace=True)\n",
    "\n",
    "def rep_fix(x):\n",
    "    if \"RP\" in str(x):\n",
    "        return x[-1]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "big_df['Reporting Period'] = big_df['Reporting Period'].apply(lambda x: rep_fix(x))\n",
    "big_df['Reporting Period'] = pd.to_numeric(big_df['Reporting Period'])\n",
    "\n",
    "big_df['Date'] = pd.to_datetime(big_df['Date'])\n",
    "\n",
    "mill_df = big_df[['Date',\n",
    "                  'Ticket',\n",
    "                  'Mill',\n",
    "                  'Product',\n",
    "                  'Tons (U.S)',\n",
    "                  'Cull Tons',\n",
    "                  'Accepted Tons',\n",
    "                  'Source',\n",
    "                  'Notes',\n",
    "                  'Reporting Period',\n",
    "                  'TicketID',\n",
    "                  'Week']].copy()\n",
    "\n",
    "mill_df = mill_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the expected harvest\n",
    "\n",
    "Find all activities that are Open, Pending, In Process, Post\n",
    "\n",
    "Get their ticket id\n",
    "\n",
    "Get the sum of the expected rp8 for that ticketid\n",
    "get the sum of the expected rp9 for that ticketid\n",
    "\n",
    "get the harvested volumes for those ticketids (already did this by getting the sums and translating from old ticketid to new ticketid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgres_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
